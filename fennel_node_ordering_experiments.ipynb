{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import platform\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from graph_partitioning import GraphPartitioning, utils\n",
    "\n",
    "cols = [\"WASTE\", \"CUT RATIO\", \"EDGES CUT\", \"TOTAL COMM VOLUME\", \"Qds\", \"CONDUCTANCE\", \"MAXPERM\", \"RBSE\", \"NMI\", \"FSCORE\", \"FSCORE RELABEL IMPROVEMENT\", \"LONELINESS\"]\n",
    "\n",
    "pwd = %pwd\n",
    "\n",
    "\n",
    "ORDERED_ARRIVALS_DIR = os.path.join(pwd, \"data\", \"ideal_node_ordering\", \"ordered_centralities\")\n",
    "\n",
    "analysisOnly = False\n",
    "\n",
    "\n",
    "# [] 15 rankings - minimal binning\n",
    "# [] \n",
    "\n",
    "\n",
    "# parametrized config\n",
    "parametrized_config = {\n",
    "    \"DATA_FILENAME\": os.path.join(pwd, \"data\", \"ideal_node_ordering\", \"edgelist\", \"nn#networkID#.txt\"),\n",
    "    \"OUTPUT_DIRECTORY\": os.path.join(pwd, \"output\", \"ideal_node_ordering\"),\n",
    "\n",
    "    # Set which algorithm is run for the PREDICTION MODEL.\n",
    "    # Either: 'FENNEL' or 'SCOTCH'\n",
    "    \"PREDICTION_MODEL_ALGORITHM\": \"FENNEL\",\n",
    "\n",
    "    # Alternativly, read input file for prediction model.\n",
    "    # Set to empty to generate prediction model using algorithm value above.\n",
    "    \"PREDICTION_MODEL\": \"\",\n",
    "\n",
    "    \n",
    "    \"PARTITIONER_ALGORITHM\": \"FENNEL\",\n",
    "\n",
    "    # File containing simulated arrivals. This is used in simulating nodes\n",
    "    # arriving at the shelter. Nodes represented by line number; value of\n",
    "    # 1 represents a node as arrived; value of 0 represents the node as not\n",
    "    # arrived or needing a shelter.\n",
    "    \"SIMULATED_ARRIVAL_FILE\": os.path.join(pwd,\n",
    "                                           \"data\",\n",
    "                                           \"predition_model_tests\",\n",
    "                                           \"dataset_1_shift_rotate\",\n",
    "                                           \"simulated_arrival_list\",\n",
    "                                           \"percentage_of_prediction_correct_#correctedness#\",\n",
    "                                           \"arrival_#correctedness#_#networkID#.txt\"\n",
    "                                          ),\n",
    "    \n",
    "    # File containing the prediction of a node arriving. This is different to the\n",
    "    # simulated arrivals, the values in this file are known before the disaster.\n",
    "    \"PREDICTION_LIST_FILE\": os.path.join(pwd,\n",
    "                                         \"data\",\n",
    "                                         \"predition_model_tests\",\n",
    "                                         \"dataset_1_shift_rotate\",\n",
    "                                         \"prediction_list\",\n",
    "                                         \"prediction_#networkID#.txt\"\n",
    "                                        ),\n",
    "\n",
    "    # File containing the geographic location of each node, in \"x,y\" format.\n",
    "    \"POPULATION_LOCATION_FILE\": os.path.join(pwd,\n",
    "                                             \"data\",\n",
    "                                             \"predition_model_tests\",\n",
    "                                             \"coordinates\",\n",
    "                                             \"coordinates_#networkID#.txt\"\n",
    "                                            ),\n",
    "\n",
    "    # Number of shelters\n",
    "    \"num_partitions\": 4,\n",
    "\n",
    "    # The number of iterations when making prediction model\n",
    "    \"num_iterations\": 3,\n",
    "\n",
    "    # Percentage of prediction model to use before discarding\n",
    "    # When set to 0, prediction model is discarded, useful for one-shot\n",
    "    \"prediction_model_cut_off\": 0.0,\n",
    "\n",
    "    # Alpha value used in one-shot (when restream_batches set to 1)\n",
    "    \"one_shot_alpha\": 0.5,\n",
    "\n",
    "    \"use_one_shot_alpha\" : False,\n",
    "\n",
    "    # Number of arrivals to batch before recalculating alpha and restreaming.\n",
    "    # When set to 1, one-shot is used with alpha value from above\n",
    "    \"restream_batches\": 50,\n",
    "\n",
    "    # When the batch size is reached: if set to True, each node is assigned\n",
    "    # individually as first in first out. If set to False, the entire batch\n",
    "    # is processed and empty before working on the next batch.\n",
    "    \"sliding_window\": False,\n",
    "\n",
    "    # Create virtual nodes based on prediction model\n",
    "    \"use_virtual_nodes\": False,\n",
    "\n",
    "    # Virtual nodes: edge weight\n",
    "    \"virtual_edge_weight\": 1.0,\n",
    "\n",
    "    # Loneliness score parameter. Used when scoring a partition by how many\n",
    "    # lonely nodes exist.\n",
    "    \"loneliness_score_param\": 1.2,\n",
    "    \n",
    "    \n",
    "    \"compute_metrics_enabled\": True,\n",
    "\n",
    "    ####\n",
    "    # GRAPH MODIFICATION FUNCTIONS\n",
    "\n",
    "    # Also enables the edge calculation function.\n",
    "    \"graph_modification_functions\": True,\n",
    "\n",
    "    # If set, the node weight is set to 100 if the node arrives at the shelter,\n",
    "    # otherwise the node is removed from the graph.\n",
    "    \"alter_arrived_node_weight_to_100\": False,\n",
    "\n",
    "    # Uses generalized additive models from R to generate prediction of nodes not\n",
    "    # arrived. This sets the node weight on unarrived nodes the the prediction\n",
    "    # given by a GAM.\n",
    "    # Needs POPULATION_LOCATION_FILE to be set.\n",
    "    \"alter_node_weight_to_gam_prediction\": False,\n",
    "\n",
    "    # The value of 'k' used in the GAM will be the number of nodes arrived until\n",
    "    # it reaches this max value.\n",
    "    \"gam_k_value\": 100,\n",
    "\n",
    "    # Alter the edge weight for nodes that haven't arrived. This is a way to\n",
    "    # de-emphasise the prediction model for the unknown nodes.\n",
    "    \"prediction_model_emphasis\": 1.0,\n",
    "    \n",
    "    # This applies the prediction_list_file node weights onto the nodes in the graph\n",
    "    # when the prediction model is being computed and then removes the weights\n",
    "    # for the cutoff and batch arrival modes\n",
    "    \"apply_prediction_model_weights\": True,\n",
    "    \n",
    "    # Path to the scotch shared library\n",
    "    \"SCOTCH_LIB_PATH\": os.path.join(pwd, \"libs/scotch/macOS/libscotch.dylib\")\n",
    "    if 'Darwin' in platform.system()\n",
    "    else \"/usr/local/lib/libscotch.so\",\n",
    "    \n",
    "    # Path to the PaToH shared library\n",
    "    \"PATOH_LIB_PATH\": os.path.join(pwd, \"libs/patoh/lib/macOS/libpatoh.dylib\")\n",
    "    if 'Darwin' in platform.system()\n",
    "    else os.path.join(pwd, \"libs/patoh/lib/linux/libpatoh.so\"),\n",
    "    \n",
    "    \"PATOH_ITERATIONS\": 5,\n",
    "        \n",
    "    # Expansion modes: 'avg_node_weight', 'total_node_weight', 'smallest_node_weight'\n",
    "    # 'largest_node_weight'\n",
    "    # add '_squared' or '_sqrt' at the end of any of the above for ^2 or sqrt(weight)\n",
    "    # i.e. 'avg_node_weight_squared\n",
    "    \"PATOH_HYPEREDGE_EXPANSION_MODE\": 'no_expansion',\n",
    "    \n",
    "    # Edge Expansion: average, total, minimum, maximum, product, product_squared, sqrt_product\n",
    "    \"EDGE_EXPANSION_MODE\" : 'total',\n",
    "    \n",
    "    # Whether nodes should be reordered using a centrality metric for optimal node assignments in batch mode\n",
    "    # This is specific to FENNEL and at the moment Leverage Centrality is used to compute new noder orders\n",
    "    \"FENNEL_NODE_REORDERING_ENABLED\": False,\n",
    "    \n",
    "    # Whether the Friend of a Friend scoring system is active during FENNEL partitioning.\n",
    "    # FOAF employs information about a node's friends to determine the best partition when\n",
    "    # this node arrives at a shelter and no shelter has friends already arrived\n",
    "    \"FENNEL_FRIEND_OF_A_FRIEND_ENABLED\": False,\n",
    "    \n",
    "    # Alters how much information to print. Keep it at 1 for this notebook.\n",
    "    # 0 - will print nothing, useful for batch operations.\n",
    "    # 1 - prints basic information on assignments and operations.\n",
    "    # 2 - prints more information as it batches arrivals.\n",
    "    \"verbose\": 0\n",
    "}\n",
    "\n",
    "#gp = GraphPartitioning(config)\n",
    "\n",
    "# Optional: shuffle the order of nodes arriving\n",
    "# Arrival order should not be shuffled if using GAM to alter node weights\n",
    "#random.shuffle(gp.arrival_order)\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA centrality, random ordering n.40 experiments\n",
      "Alpha centrality, HL ordering n.40 experiments\n",
      "Alpha centrality, LH ordering n.40 experiments\n",
      "Average distance centrality, HL ordering n.40 experiments\n",
      "Average distance centrality, LH ordering n.40 experiments\n",
      "Barycenter centrality centrality, HL ordering n.40 experiments\n",
      "Barycenter centrality centrality, LH ordering n.40 experiments\n",
      "Betweenness centrality, HL ordering n.40 experiments\n",
      "Betweenness centrality, LH ordering n.40 experiments\n",
      "BottleNeck centrality centrality, HL ordering n.40 experiments\n",
      "BottleNeck centrality centrality, LH ordering n.40 experiments\n",
      "Bridging centrality centrality, HL ordering n.40 experiments\n",
      "Bridging centrality centrality, LH ordering n.40 experiments\n",
      "Centroid centrality centrality, HL ordering n.40 experiments\n",
      "Centroid centrality centrality, LH ordering n.40 experiments\n",
      "Closeness Freeman centrality, HL ordering n.40 experiments\n",
      "Closeness Freeman centrality, LH ordering n.40 experiments\n",
      "Closeness VariantLatora centrality, HL ordering n.40 experiments\n",
      "Closeness VariantLatora centrality, LH ordering n.40 experiments\n",
      "ClusterRank centrality, HL ordering n.40 experiments\n",
      "ClusterRank centrality, LH ordering n.40 experiments\n",
      "Communicability betweenness centrality centrality, HL ordering n.40 experiments\n",
      "Communicability betweenness centrality centrality, LH ordering n.40 experiments\n",
      "Community centrality centrality, HL ordering n.40 experiments\n",
      "Community centrality centrality, LH ordering n.40 experiments\n",
      "Core decomposition centrality, HL ordering n.40 experiments\n",
      "Core decomposition centrality, LH ordering n.40 experiments\n",
      "Cross clique centrality centrality, LH ordering n.40 experiments\n",
      "Cross clique connectivity centrality, HL ordering n.40 experiments\n",
      "Current flow closeness centrality centrality, HL ordering n.40 experiments\n",
      "Current flow closeness centrality centrality, LH ordering n.40 experiments\n",
      "Dangalchev closeness centrality centrality, HL ordering n.40 experiments\n",
      "Dangalchev closeness centrality centrality, LH ordering n.40 experiments\n",
      "Decay centrality centrality, HL ordering n.40 experiments\n",
      "Decay centrality centrality, LH ordering n.40 experiments\n",
      "Degree centrality centrality, HL ordering n.40 experiments\n",
      "Degree centrality centrality, LH ordering n.40 experiments\n",
      "Diffusion degree centrality, HL ordering n.40 experiments\n",
      "Diffusion degree centrality, LH ordering n.40 experiments\n",
      "DMNC centrality centrality, HL ordering n.40 experiments\n",
      "DMNC centrality centrality, LH ordering n.40 experiments\n",
      "Eccentricity centrality, HL ordering n.40 experiments\n",
      "Eccentricity centrality, LH ordering n.40 experiments\n",
      "Effectiveness centrality centrality, HL ordering n.40 experiments\n",
      "Effectiveness centrality centrality, LH ordering n.40 experiments\n",
      "Eigenvector centrality, HL ordering n.40 experiments\n",
      "Eigenvector centrality, LH ordering n.40 experiments\n",
      "Entropy centrality centrality, HL ordering n.40 experiments\n",
      "Entropy centrality centrality, LH ordering n.40 experiments\n",
      "EPC centrality, HL ordering n.40 experiments\n",
      "EPC centrality, LH ordering n.40 experiments\n",
      "Flow betweenness centrality centrality, HL ordering n.40 experiments\n",
      "Flow betweenness centrality centrality, LH ordering n.40 experiments\n",
      "Information centrality centrality, HL ordering n.40 experiments\n",
      "Information centrality centrality, LH ordering n.40 experiments\n",
      "Kleinbergs centrality HITS centrality, HL ordering n.40 experiments\n",
      "Kleinbergs centrality HITS centrality, LH ordering n.40 experiments\n",
      "LAC centrality, HL ordering n.40 experiments\n",
      "LAC centrality, LH ordering n.40 experiments\n",
      "Lapacian centrality centrality, HL ordering n.40 experiments\n",
      "Lapacian centrality centrality, LH ordering n.40 experiments\n",
      "Leverage centrality centrality, HL ordering n.40 experiments\n",
      "Leverage centrality centrality, LH ordering n.40 experiments\n",
      "Lin centrality centrality, HL ordering n.40 experiments\n",
      "Lin centrality centrality, LH ordering n.40 experiments\n",
      "Load centrality centrality, HL ordering n.40 experiments\n",
      "Load centrality centrality, LH ordering n.40 experiments\n",
      "Lobby index centrality, HL ordering n.40 experiments\n",
      "Lobby index centrality, LH ordering n.40 experiments\n",
      "Local assortativity centrality, HL ordering n.40 experiments\n",
      "Local assortativity centrality, LH ordering n.40 experiments\n",
      "Local clustering coefficients centrality, HL ordering n.40 experiments\n",
      "Local clustering coefficients centrality, LH ordering n.40 experiments\n",
      "Markov centrality centrality, HL ordering n.40 experiments\n",
      "Markov centrality centrality, LH ordering n.40 experiments\n",
      "MCC centrality centrality, HL ordering n.40 experiments\n",
      "MCC centrality centrality, LH ordering n.40 experiments\n",
      "MNC centrality centrality, HL ordering n.40 experiments\n",
      "MNC centrality centrality, LH ordering n.40 experiments\n",
      "Neighborhood connectivity centrality, HL ordering n.40 experiments\n",
      "Neighborhood connectivity centrality, LH ordering n.40 experiments\n",
      "Network centrality centrality, HL ordering n.40 experiments\n",
      "Network centrality centrality, LH ordering n.40 experiments\n",
      "Network fragmentation GeodesicDistanceWeighted centrality, HL ordering n.40 experiments\n",
      "Network fragmentation GeodesicDistanceWeighted centrality, LH ordering n.40 experiments\n",
      "Network fragmentation centrality, HL ordering n.40 experiments\n",
      "Network fragmentation centrality, LH ordering n.40 experiments\n",
      "Path centrality centrality, HL ordering n.40 experiments\n",
      "Path centrality centrality, LH ordering n.40 experiments\n",
      "Political independence index centrality, HL ordering n.40 experiments\n",
      "Political independence index centrality, LH ordering n.40 experiments\n",
      "Radiality centrality centrality, HL ordering n.40 experiments\n",
      "Radiality centrality centrality, LH ordering n.40 experiments\n",
      "Random walk betweenness centrality, HL ordering n.40 experiments\n",
      "Random walk betweenness centrality, LH ordering n.40 experiments\n",
      "Random walk closeness centrality, HL ordering n.40 experiments\n",
      "Random walk closeness centrality, LH ordering n.40 experiments\n",
      "SALSA centrality, HL ordering n.40 experiments\n",
      "SALSA centrality, LH ordering n.40 experiments\n",
      "Semi local centrality centrality, HL ordering n.40 experiments\n",
      "Semi local centrality centrality, LH ordering n.40 experiments\n",
      "Shortest path betweenness centrality, HL ordering n.40 experiments\n",
      "Shortest path betweenness centrality, LH ordering n.40 experiments\n",
      "Shortest path closeness centrality, HL ordering n.40 experiments\n",
      "Shortest path closeness centrality, LH ordering n.40 experiments\n",
      "Shortest path degree centrality, HL ordering n.40 experiments\n",
      "Shortest path degree centrality, LH ordering n.40 experiments\n",
      "Strength weighted vertex degree centrality, HL ordering n.40 experiments\n",
      "Strength weighted vertex degree centrality, LH ordering n.40 experiments\n",
      "Stress centrality centrality, HL ordering n.40 experiments\n",
      "Stress centrality centrality, LH ordering n.40 experiments\n",
      "Subgraph centrality, HL ordering n.40 experiments\n",
      "Subgraph centrality, LH ordering n.40 experiments\n",
      "Topological coefficient centrality, HL ordering n.40 experiments\n",
      "Topological coefficient centrality, LH ordering n.40 experiments\n"
     ]
    }
   ],
   "source": [
    "# load the centralities files\n",
    "import scipy.stats as sstats\n",
    "centralities = {}\n",
    "\n",
    "class CentralitiesExperiment:\n",
    "    def __init__(self, dirName, dataPath):\n",
    "        self.dirName = dirName\n",
    "        self.dataPath = dataPath\n",
    "        self.outputPath = os.path.join(parametrized_config[\"OUTPUT_DIRECTORY\"], dirName)\n",
    "                \n",
    "        parts = dirName.split(\"_\")\n",
    "        \n",
    "        self.orderType = parts[len(parts) - 1]\n",
    "        \n",
    "        name = dirName.replace('_' + self.orderType, \"\")\n",
    "        self.centralityType = name.replace(\"_\", \" \")\n",
    "\n",
    "        self.experimentFileNames = []\n",
    "        self.experimentFilePaths = []\n",
    "        \n",
    "        self.scores = []\n",
    "        self.avgScores = []\n",
    "        self.varScores = []\n",
    "        self.stdScores = []\n",
    "        self.skewnessScores = []\n",
    "        self.modeScores = []\n",
    "        \n",
    "        self.totalScore = 0.0\n",
    "\n",
    "    def computeStatsScore(self):\n",
    "        if(len(self.scores) == 0):\n",
    "            return\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        self.avgScores = []\n",
    "        self.varScores = []\n",
    "        self.stdScores = []\n",
    "        self.skewnessScores = []\n",
    "        self.modeScores = []\n",
    "\n",
    "        for i in range(0, len(self.scores[0])):\n",
    "            scores.append([])\n",
    "            self.avgScores.append(0.0)\n",
    "            self.varScores.append(0.0)\n",
    "            self.stdScores.append(0.0)\n",
    "            self.skewnessScores.append(0.0)\n",
    "            self.modeScores.append(0.0)\n",
    "        \n",
    "        for score in self.scores:\n",
    "            for i, val in enumerate(score):\n",
    "                scores[i].append(float(val))\n",
    "        \n",
    "        for i, data in enumerate(scores):\n",
    "            data = np.array(data)\n",
    "            self.avgScores[i] = sstats.tmean(data)\n",
    "            self.varScores[i] = sstats.tvar(data)\n",
    "            self.stdScores[i] = sstats.tstd(data)\n",
    "            self.skewnessScores[i] = sstats.skew(data)\n",
    "            mode = sstats.mode(data)\n",
    "            self.modeScores[i] = str(mode[0][0]) + \":\" + str(mode[1][0])\n",
    "                \n",
    "    def _computeStatsScore(self):\n",
    "        if(len(self.scores) == 0):\n",
    "            return\n",
    "        \n",
    "        self.avgScores = []\n",
    "        self.varScores = []\n",
    "        self.stdScores = []\n",
    "        \n",
    "        for i in range(0, len(self.scores[0])):\n",
    "            self.avgScores.append(0.0)\n",
    "            self.varScores.append(0.0)\n",
    "        \n",
    "        for score in self.scores:\n",
    "            for i, val in enumerate(score):\n",
    "                self.avgScores[i] = self.avgScores[i] + float(val)\n",
    "        \n",
    "        for i, total in enumerate(self.avgScores):\n",
    "            self.avgScores[i] = total / len(self.scores)\n",
    "            \n",
    "        # compute variance\n",
    "        for score in self.scores:\n",
    "            for i, val in enumerate(score):\n",
    "                mean = self.avgScores[i]\n",
    "                diffsquared = (float(val) - mean) ** 2\n",
    "                self.varScores[i] = self.varScores[i] + diffsquared\n",
    "        \n",
    "        for i, total in enumerate(self.varScores):\n",
    "            self.varScores[i] = total / len(self.scores)\n",
    "            self.stdScores.append(self.varScores[i] ** 0.5)\n",
    "            \n",
    "    def printScoreline(self, scoreline):\n",
    "        print(\"{0:.5f}\\t{1:.10f}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\".format(scoreline[0],scoreline[1],scoreline[2],scoreline[3],scoreline[4],scoreline[5],scoreline[6]))\n",
    "            \n",
    "    def saveScores(self):\n",
    "        try:\n",
    "            os.makedirs(self.outputPath)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        fName = os.path.join(self.outputPath, \"scores.txt\")\n",
    "        with open(fName, 'w+') as f:\n",
    "            f.write(self.scoreStr(self.avgScores) + \"\\n\")\n",
    "            f.write(self.scoreStr(self.varScores) + \"\\n\")\n",
    "            f.write(self.scoreStr(self.stdScores) + \"\\n\")\n",
    "            f.write(self.scoreStr(self.modeScores) + \"\\n\")\n",
    "            f.write(self.scoreStr(self.skewnessScores) + \"\\n\")\n",
    "            \n",
    "            for score in self.scores:\n",
    "                f.write(self.scoreStr(score) + \"\\n\")\n",
    "\n",
    "    def loadScores(self):\n",
    "        self.scores = []\n",
    "        fName = os.path.join(self.outputPath, \"scores.txt\")\n",
    "        with open(fName, 'r') as f:\n",
    "            count = 0\n",
    "            for line in f:\n",
    "                if(count < 5):\n",
    "                    count += 1\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                parts = line.split(',')\n",
    "                score = []\n",
    "                for part in parts:\n",
    "                    score.append(float(part))\n",
    "                self.scores.append(score)\n",
    "                \n",
    "    def scoreStr(self, score):\n",
    "        s = \"\"\n",
    "        for val in score:\n",
    "            if len(s) > 0:\n",
    "                s = s + \",\"\n",
    "            s = s + str(val)\n",
    "        return s\n",
    "            \n",
    "    def centrality(self):\n",
    "        return self.centralityType\n",
    "    \n",
    "    def ordering(self):\n",
    "        return self.orderType\n",
    "    \n",
    "    def metadata(self):\n",
    "        return self.centralityType + \" centrality, \" + self.orderType + \" ordering n.\" + str(self.numExperiments()) + \" experiments\"\n",
    "    \n",
    "    def loadExperimentFiles(self):\n",
    "        for root, dirs, files in os.walk(self.dataPath):\n",
    "            for file in files:\n",
    "                if(file.endswith(\".txt\")):\n",
    "                    self.experimentFileNames.append(file.split(\".txt\")[0])\n",
    "        centrality.sortExperiments()\n",
    "\n",
    "    def sortExperiments(self):\n",
    "        ordered = []\n",
    "        indeces = {}\n",
    "        for f in self.experimentFileNames:\n",
    "            parts = f.split(\"_\")\n",
    "            index = int(parts[len(parts) - 1])\n",
    "            indeces[index] = f\n",
    "        \n",
    "        self.experimentFileNames = []\n",
    "\n",
    "        for i, idx in enumerate(sorted(list(indeces.keys()))):\n",
    "            fn = indeces[idx]\n",
    "            self.experimentFileNames.append(fn)\n",
    "            self.experimentFilePaths.append(os.path.join(self.dataPath, fn) + \".txt\")\n",
    "\n",
    "    def numExperiments(self):\n",
    "        return len(self.experimentFilePaths)\n",
    "            \n",
    "    def getDataExperimentPath(self, experimentNumber):\n",
    "        '''must go from 0 - n'''\n",
    "        if(experimentNumber >= 0 and experimentNumber < len(self.experimentFilePaths)):\n",
    "            return self.experimentFilePaths[experimentNumber]\n",
    "        return \"\"\n",
    "    \n",
    "    def getOutputExperimentPath(self, experimentNumber):\n",
    "        '''must go from 0 - n'''\n",
    "        if(experimentNumber >= 0 and experimentNumber < len(self.experimentFileNames)):\n",
    "            experiment = self.experimentFileNames[experimentNumber].split(\".txt\")\n",
    "            outFile = experiment[0] + \"_out.txt\"\n",
    "            return os.path.join(self.outputPath, outFile)\n",
    "        return \"\"\n",
    "        \n",
    "    def print(self):\n",
    "        print(self.dataPath)\n",
    "        for i, f in enumerate(self.experimentFileNames):\n",
    "            print(i,\" \", f)\n",
    "            print(self.experimentFilePaths[i])\n",
    "\n",
    "dataFiles = []\n",
    "for i in range(1, 41):\n",
    "    dataFiles.append(parametrized_config['DATA_FILENAME'].replace(\"#networkID#\", str(i)))\n",
    "# traverse root directory, and list directories as dirs and files as files\n",
    "for root, dirs, files in os.walk(ORDERED_ARRIVALS_DIR):\n",
    "    for directory in dirs:\n",
    "        centrality = CentralitiesExperiment(directory, os.path.join(ORDERED_ARRIVALS_DIR, directory))\n",
    "        centrality.loadExperimentFiles()\n",
    "        centralities[directory] = centrality\n",
    "        print(centrality.metadata())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: AA centrality, random ordering n.40 experiments\n",
      "Running experiment: Alpha centrality, HL ordering n.40 experiments\n",
      "Running experiment: Alpha centrality, LH ordering n.40 experiments\n",
      "Running experiment: Average distance centrality, HL ordering n.40 experiments\n",
      "Running experiment: Average distance centrality, LH ordering n.40 experiments\n",
      "Running experiment: Barycenter centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Barycenter centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Betweenness centrality, HL ordering n.40 experiments\n",
      "Running experiment: Betweenness centrality, LH ordering n.40 experiments\n",
      "Running experiment: BottleNeck centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: BottleNeck centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Bridging centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Bridging centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Centroid centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Centroid centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Closeness Freeman centrality, HL ordering n.40 experiments\n",
      "Running experiment: Closeness Freeman centrality, LH ordering n.40 experiments\n",
      "Running experiment: Closeness VariantLatora centrality, HL ordering n.40 experiments\n",
      "Running experiment: Closeness VariantLatora centrality, LH ordering n.40 experiments\n",
      "Running experiment: ClusterRank centrality, HL ordering n.40 experiments\n",
      "Running experiment: ClusterRank centrality, LH ordering n.40 experiments\n",
      "Running experiment: Communicability betweenness centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Communicability betweenness centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Community centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Community centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Core decomposition centrality, HL ordering n.40 experiments\n",
      "Running experiment: Core decomposition centrality, LH ordering n.40 experiments\n",
      "Running experiment: Cross clique centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Cross clique connectivity centrality, HL ordering n.40 experiments\n",
      "Running experiment: Current flow closeness centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Current flow closeness centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Dangalchev closeness centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Dangalchev closeness centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Decay centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Decay centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Degree centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Degree centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Diffusion degree centrality, HL ordering n.40 experiments\n",
      "Running experiment: Diffusion degree centrality, LH ordering n.40 experiments\n",
      "Running experiment: DMNC centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: DMNC centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Eccentricity centrality, HL ordering n.40 experiments\n",
      "Running experiment: Eccentricity centrality, LH ordering n.40 experiments\n",
      "Running experiment: Effectiveness centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Effectiveness centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Eigenvector centrality, HL ordering n.40 experiments\n",
      "Running experiment: Eigenvector centrality, LH ordering n.40 experiments\n",
      "Running experiment: Entropy centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Entropy centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: EPC centrality, HL ordering n.40 experiments\n",
      "Running experiment: EPC centrality, LH ordering n.40 experiments\n",
      "Running experiment: Flow betweenness centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Flow betweenness centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Information centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Information centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Kleinbergs centrality HITS centrality, HL ordering n.40 experiments\n",
      "Running experiment: Kleinbergs centrality HITS centrality, LH ordering n.40 experiments\n",
      "Running experiment: LAC centrality, HL ordering n.40 experiments\n",
      "Running experiment: LAC centrality, LH ordering n.40 experiments\n",
      "Running experiment: Lapacian centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Lapacian centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Leverage centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Leverage centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Lin centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Lin centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Load centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Load centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Lobby index centrality, HL ordering n.40 experiments\n",
      "Running experiment: Lobby index centrality, LH ordering n.40 experiments\n",
      "Running experiment: Local assortativity centrality, HL ordering n.40 experiments\n",
      "Running experiment: Local assortativity centrality, LH ordering n.40 experiments\n",
      "Running experiment: Local clustering coefficients centrality, HL ordering n.40 experiments\n",
      "Running experiment: Local clustering coefficients centrality, LH ordering n.40 experiments\n",
      "Running experiment: Markov centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Markov centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: MCC centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: MCC centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: MNC centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: MNC centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Neighborhood connectivity centrality, HL ordering n.40 experiments\n",
      "Running experiment: Neighborhood connectivity centrality, LH ordering n.40 experiments\n",
      "Running experiment: Network centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Network centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Network fragmentation GeodesicDistanceWeighted centrality, HL ordering n.40 experiments\n",
      "Running experiment: Network fragmentation GeodesicDistanceWeighted centrality, LH ordering n.40 experiments\n",
      "Running experiment: Network fragmentation centrality, HL ordering n.40 experiments\n",
      "Running experiment: Network fragmentation centrality, LH ordering n.40 experiments\n",
      "Running experiment: Path centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Path centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Political independence index centrality, HL ordering n.40 experiments\n",
      "Running experiment: Political independence index centrality, LH ordering n.40 experiments\n",
      "Running experiment: Radiality centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Radiality centrality centrality, LH ordering n.40 experiments\n",
      "Running experiment: Random walk betweenness centrality, HL ordering n.40 experiments\n",
      "Running experiment: Random walk betweenness centrality, LH ordering n.40 experiments\n",
      "Running experiment: Random walk closeness centrality, HL ordering n.40 experiments\n",
      "Running experiment: Random walk closeness centrality, LH ordering n.40 experiments\n",
      "Running experiment: SALSA centrality, HL ordering n.40 experiments\n",
      "Running experiment: SALSA centrality, LH ordering n.40 experiments\n",
      "Running experiment: Semi local centrality centrality, HL ordering n.40 experiments\n",
      "Running experiment: Semi local centrality centrality, LH ordering n.40 experiments\n"
     ]
    }
   ],
   "source": [
    "# run the experiments here\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "from graph_partitioning import fennel as fnl\n",
    "from graph_partitioning import GraphPartitioning, utils\n",
    "\n",
    "fennel = fnl.FennelPartitioner(0.5)\n",
    "\n",
    "def loadGraph(edgeFile):\n",
    "    G = nx.Graph()\n",
    "    edges = []\n",
    "    with open(edgeFile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.split(\" \")\n",
    "            n1 = int(line[0])\n",
    "            n2 = int(line[1])\n",
    "            \n",
    "            G.add_node(n1)\n",
    "            G.add_node(n2)\n",
    "            \n",
    "            edges.append((n1, n2))\n",
    "        for edge in edges:\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "    nx.set_node_attributes(G, 'weight', 1.0)\n",
    "    nx.set_edge_attributes(G, 'weight', 1.0)\n",
    "    return G\n",
    "\n",
    "def loadArrivals(arrivalFile):\n",
    "    arrivals = []\n",
    "    with open(arrivalFile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            arrivals.append(int(line))\n",
    "    return np.array(arrivals, dtype=np.int32)\n",
    "\n",
    "def generateArray(num, value):\n",
    "    arr = []\n",
    "    for i in range(0, num):\n",
    "        arr.append(value)\n",
    "    return np.array(arr, dtype=np.int32)\n",
    "\n",
    "def checkInputData(graph, arrivals):\n",
    "    if(graph.number_of_nodes() == 0):\n",
    "        print(\"Error, no nodes\")\n",
    "        return False\n",
    "    if(graph.number_of_edges() == 0):\n",
    "        print(\"Error, no edges\")\n",
    "        return False\n",
    "        \n",
    "    arr = np.array(arrivals)\n",
    "    if(np.min(arr) > 0):\n",
    "        print(\"Error arrival file has minimum node ID > 0:\", np.min(arr))\n",
    "        return False\n",
    "    if(np.max(arr) >= graph.number_of_nodes()):\n",
    "        print(\"Error arrival file has maximum node ID >= number_of_nodes():\", np.max(arr))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def computeAlpha(graph, num_partitions):\n",
    "    numedges = graph.number_of_edges()\n",
    "    if(graph.is_directed()):\n",
    "        numedges = numedges * 0.5\n",
    "    return numedges * (num_partitions / (graph.number_of_nodes()**2))\n",
    "    \n",
    "def printScore(graph, assignments, num_partitions, loneliness_score_param, verbose = 1):\n",
    "        x = utils.score(graph, assignments, num_partitions)\n",
    "        edges_cut, steps, cut_edges = utils.base_metrics(graph, assignments)\n",
    "\n",
    "        q_qds_conductance = utils.infomapModularityComQuality(graph, assignments, num_partitions)\n",
    "        #old: mod = utils.modularity_wavg(graph, assignments, num_partitions)\n",
    "        loneliness = utils.loneliness_score_wavg(graph, loneliness_score_param, assignments, num_partitions)\n",
    "        max_perm = utils.wavg_max_perm(graph, assignments, num_partitions)\n",
    "        #old: max_perm = utils.run_max_perm(graph)\n",
    "\n",
    "        #nmi_score = nmi_metrics.nmi(np.array([self.assignments_prediction_model, self.assignments]))\n",
    "        #nmi_score = normalized_mutual_info_score(self.assignments_prediction_model.tolist(), self.assignments.tolist())\n",
    "        if verbose > 1:\n",
    "            print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\\t\\t\\t{4}\\t{5}\\t{6}\".format(x[0], x[1], edges_cut, steps, q_qds_conductance[0], loneliness, max_perm))\n",
    "            #print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\\t\\t\\t{4}\\t{5}\\t{6}\".format(x[0], x[1], edges_cut, steps, mod, loneliness, max_perm))\n",
    "            #print(\"{0:.5f}\\t\\t{1:.10f}\\t{2}\\t\\t{3}\\t\\t\\t{4}\\t{5}\\t{6}\\t{7:.10f}\".format(x[0], x[1], edges_cut, steps, mod, loneliness, max_perm, nmi_score))\n",
    "        # waste, cut_ratio, edges_cut, TCV (steps), Qds, loneliness, max_perm\n",
    "        return [x[0], x[1], edges_cut, steps, q_qds_conductance[0], loneliness, max_perm]\n",
    "\n",
    "\n",
    "    \n",
    "# Run the centralities experiment for eachdatapoint\n",
    "'''for key in list(centralities.keys()):\n",
    "    if analysisOnly == True:\n",
    "        break\n",
    "\n",
    "\n",
    "    centrality = centralities[key]\n",
    "\n",
    "    print(\"Running experiment:\", centrality.metadata())\n",
    "\n",
    "    for i in range(0, 40):\n",
    "        with GraphPartitioning(parametrized_config) as gp:\n",
    "            gp.verbose = 0\n",
    "            gp.DATA_FILENAME = dataFiles[i]\n",
    "            print(gp.DATA_FILENAME)\n",
    "            \n",
    "            gp.load_network()\n",
    "            gp.init_partitioner()\n",
    "    \n",
    "            gp.arrival_order = loadArrivals(centrality.getDataExperimentPath(i))\n",
    "    \n",
    "            m = gp.prediction_model()\n",
    "            m = gp.assign_cut_off()\n",
    "            m = gp.batch_arrival()\n",
    "            \n",
    "            print(m)\n",
    "        break\n",
    "\n",
    "analysisOnly = True\n",
    "'''\n",
    "\n",
    "# FORMAT OF SAVED scores.txt files\n",
    "\n",
    "# average (scores)\n",
    "# variance (scores)\n",
    "# std (scores)\n",
    "# mode(scores)\n",
    "# skewness (scores)\n",
    "# this is then followed by each experiment's scores over which the stats above are computed\n",
    "# waste, cut_ratio, edges_cut, TCV (steps), Qds, loneliness, max_perm\n",
    "\n",
    "for key in list(centralities.keys()):\n",
    "    if analysisOnly == True:\n",
    "        break\n",
    "    \n",
    "    centrality = centralities[key]\n",
    "\n",
    "    print(\"Running experiment:\", centrality.metadata())\n",
    "\n",
    "    for i in range(0, 40):\n",
    "        edgeFile = dataFiles[i]\n",
    "        \n",
    "        G = loadGraph(edgeFile)\n",
    "        arrival_list = loadArrivals(centrality.getDataExperimentPath(i))\n",
    "        GSub = G.subgraph(arrival_list)\n",
    "        \n",
    "        if checkInputData(G, arrival_list):\n",
    "            # ok, can proceed\n",
    "            assignments = generateArray(G.number_of_nodes(), -1)\n",
    "            fixed = generateArray(G.number_of_nodes(), -1)\n",
    "            assignments = fennel.generate_prediction_model(GSub, parametrized_config['num_iterations'], parametrized_config['num_partitions'], assignments, fixed)\n",
    "            # score contains: x[0], x[1], edges_cut, steps, mod, loneliness, max_perm\n",
    "            score = printScore(GSub, assignments, parametrized_config['num_partitions'], parametrized_config['loneliness_score_param'])\n",
    "            centrality.scores.append(score)\n",
    "    centrality.computeStatsScore()\n",
    "    centrality.saveScores()\n",
    "print(\"Finished experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: AA:random\n",
      "0.07200\t0.3760199409\t93.025\t112.425\t0.4296177517811957\t0.694817851014575\t0.016837632999999998\n",
      "Experiment: Alpha:HL\n",
      "0.08700\t0.3992272153\t98.775\t116.275\t0.4526724864713384\t0.688844495978325\t0.04471363175\n",
      "Experiment: Alpha:LH\n",
      "0.08300\t0.3668786000\t90.75\t113.65\t0.4469434667523018\t0.676137986878725\t0.003887537500000001\n",
      "Experiment: Average distance:HL\n",
      "0.07900\t0.4186254225\t103.6\t117.375\t0.4355943540076767\t0.709481980797675\t0.05500940325000001\n",
      "Experiment: Average distance:LH\n",
      "0.04100\t0.3862837419\t95.575\t113.65\t0.4240828144658379\t0.712741550880325\t0.05890800375000001\n",
      "Experiment: Barycenter centrality:HL\n",
      "0.03900\t0.3889134399\t96.2\t113.875\t0.42889015493779115\t0.7136833561709249\t0.0595434375\n",
      "Experiment: Barycenter centrality:LH\n",
      "0.07900\t0.4188194888\t103.65\t117.925\t0.43555391888629835\t0.704898162380125\t0.050251043249999995\n",
      "Experiment: Betweenness:HL\n",
      "0.05100\t0.3572308057\t88.35\t107.0\t0.4265100277584617\t0.7219089074323\t0.053563267000000005\n",
      "Experiment: Betweenness:LH\n",
      "0.08600\t0.4162378378\t102.975\t117.875\t0.421505296592643\t0.6972999528562751\t0.0452965815\n",
      "Experiment: BottleNeck centrality:HL\n",
      "0.05400\t0.3559559409\t88.075\t105.525\t0.41260803069199153\t0.726594647477825\t0.064296744\n",
      "Experiment: BottleNeck centrality:LH\n",
      "0.08500\t0.4170087440\t103.15\t118.975\t0.4320939557728831\t0.69676848395475\t0.04382970574999999\n",
      "Experiment: Bridging centrality:HL\n",
      "0.05100\t0.3981826829\t98.525\t112.95\t0.4365797098231674\t0.71581509937505\t0.0452523435\n",
      "Experiment: Bridging centrality:LH\n",
      "0.06000\t0.3849595656\t95.225\t114.8\t0.41815786133938176\t0.704851264262625\t0.06556947375\n",
      "Experiment: Centroid centrality:HL\n",
      "0.04900\t0.3753690119\t92.85\t110.675\t0.42032769641724854\t0.723575548409075\t0.06805912524999999\n",
      "Experiment: Centroid centrality:LH\n",
      "0.07900\t0.4189657140\t103.65\t118.975\t0.43272908254365544\t0.703591102378075\t0.053737364249999996\n",
      "Experiment: Closeness Freeman:HL\n",
      "0.03900\t0.3889134399\t96.2\t113.875\t0.42889015493779115\t0.7136833561709249\t0.0595434375\n",
      "Experiment: Closeness Freeman:LH\n",
      "0.07900\t0.4188194888\t103.65\t117.925\t0.43555391888629835\t0.704898162380125\t0.050251043249999995\n",
      "Experiment: Closeness VariantLatora:HL\n",
      "0.03900\t0.3842873320\t95.075\t112.7\t0.42001139440875584\t0.7140742677767\t0.053502822500000005\n",
      "Experiment: Closeness VariantLatora:LH\n",
      "0.08500\t0.4204071264\t104.05\t119.225\t0.4391800813038945\t0.7030001650175001\t0.050940929499999996\n",
      "Experiment: ClusterRank:HL\n",
      "0.04400\t0.3877894840\t95.975\t114.7\t0.42475495047050665\t0.7168989248977751\t0.04768254325\n",
      "Experiment: ClusterRank:LH\n",
      "0.04900\t0.4114405038\t101.825\t114.425\t0.43478595014320476\t0.709202151312175\t0.04545263125\n",
      "Experiment: Communicability betweenness centrality:HL\n",
      "0.04500\t0.3704039816\t91.65\t108.95\t0.4169881194519398\t0.723350890587825\t0.06948917725\n",
      "Experiment: Communicability betweenness centrality:LH\n",
      "0.08100\t0.4188548437\t103.625\t117.675\t0.43616126609558137\t0.702351674956175\t0.035060970749999996\n",
      "Experiment: Community centrality:HL\n",
      "0.04200\t0.3669617030\t90.85\t110.325\t0.4081453985146205\t0.7203688252256499\t0.06290967725\n",
      "Experiment: Community centrality:LH\n",
      "0.08300\t0.4194779510\t103.775\t118.825\t0.4352134867536478\t0.700982690931475\t0.044705112750000005\n",
      "Experiment: Core decomposition:HL\n",
      "0.04600\t0.3914544164\t96.825\t114.625\t0.418139398928966\t0.7264783901392\t0.07197687125\n",
      "Experiment: Core decomposition:LH\n",
      "0.07500\t0.4252897911\t105.225\t120.1\t0.43878618200784913\t0.700538563947125\t0.05397025825\n",
      "Experiment: Cross clique centrality:LH\n",
      "0.07300\t0.4307452747\t106.55\t120.5\t0.4365608979055306\t0.6987643939627249\t0.0491836625\n",
      "Experiment: Cross clique connectivity:HL\n",
      "0.04600\t0.3769125746\t93.225\t111.675\t0.4119977584577317\t0.72561209433825\t0.06315357625\n",
      "Experiment: Current flow closeness centrality:HL\n",
      "0.04400\t0.3786444049\t93.675\t111.1\t0.41603303436373407\t0.7229581178697749\t0.059036518999999996\n",
      "Experiment: Current flow closeness centrality:LH\n",
      "0.07500\t0.4229191903\t104.6\t118.375\t0.44042325455375125\t0.7057038386466999\t0.0480359405\n",
      "Experiment: Dangalchev closeness centrality:HL\n",
      "0.03900\t0.3863709535\t95.6\t114.2\t0.427051296033254\t0.7123292090108\t0.06447178850000002\n",
      "Experiment: Dangalchev closeness centrality:LH\n",
      "0.08600\t0.4202211214\t103.95\t118.15\t0.4291019583918191\t0.7039833297731499\t0.04577063000000001\n",
      "Experiment: Decay centrality:HL\n",
      "0.03900\t0.3863709535\t95.6\t114.2\t0.427051296033254\t0.7123292090108\t0.06447178850000002\n",
      "Experiment: Decay centrality:LH\n",
      "0.08600\t0.4202211214\t103.95\t118.15\t0.4291019583918191\t0.7039833297731499\t0.04577063000000001\n",
      "Experiment: Degree centrality:HL\n",
      "0.04900\t0.3691095561\t91.35\t109.925\t0.41095784669826363\t0.7278331069485\t0.07281501375\n",
      "Experiment: Degree centrality:LH\n",
      "0.07900\t0.4266983415\t105.6\t120.025\t0.4463369305439291\t0.6962742738898001\t0.04190105725\n",
      "Experiment: Diffusion degree:HL\n",
      "0.04300\t0.3840211397\t95.025\t113.625\t0.4210594191268958\t0.719086496546725\t0.06309244174999999\n",
      "Experiment: Diffusion degree:LH\n",
      "0.09200\t0.4218350591\t104.275\t118.275\t0.43411440734613266\t0.704267593179825\t0.03727677925\n",
      "Experiment: DMNC centrality:HL\n",
      "0.04300\t0.3987889793\t98.65\t113.925\t0.4171595122048511\t0.72170894388465\t0.03818954175\n",
      "Experiment: DMNC centrality:LH\n",
      "0.06300\t0.4076758590\t100.85\t118.425\t0.4277723610784225\t0.7038787156846\t0.05786105525\n",
      "Experiment: Eccentricity:HL\n",
      "0.06800\t0.4084691922\t101.05\t117.225\t0.42842565067928196\t0.7031299609677\t0.051263399249999994\n",
      "Experiment: Eccentricity:LH\n",
      "0.04600\t0.3884322214\t96.075\t116.2\t0.4240025022042497\t0.70501194726945\t0.045527469\n",
      "Experiment: Effectiveness centrality:HL\n",
      "0.04900\t0.3594001083\t88.925\t107.4\t0.4152410835248254\t0.72939424167025\t0.05798763725\n",
      "Experiment: Effectiveness centrality:LH\n",
      "0.08900\t0.4181885289\t103.425\t119.05\t0.4295952122744879\t0.686928003632525\t0.024196098999999992\n",
      "Experiment: Eigenvector:HL\n",
      "0.03900\t0.3958971763\t97.975\t116.2\t0.432254912120427\t0.7087221885151249\t0.045921209\n",
      "Experiment: Eigenvector:LH\n",
      "0.07400\t0.4254289676\t105.2\t118.175\t0.4431379803986412\t0.704360419390875\t0.047495934000000004\n",
      "Experiment: Entropy centrality:HL\n",
      "0.04300\t0.4081718576\t100.975\t118.55\t0.42334055204400345\t0.6953607819220751\t0.044446902749999996\n",
      "Experiment: Entropy centrality:LH\n",
      "0.07700\t0.3843711428\t95.1\t111.975\t0.4251114003681911\t0.7188922089072\t0.05062954925\n",
      "Experiment: EPC:HL\n",
      "0.04600\t0.3777898982\t93.5\t111.2\t0.41662771401352783\t0.726800734960125\t0.053983733499999985\n",
      "Experiment: EPC:LH\n",
      "0.08500\t0.4243145315\t104.95\t117.875\t0.44797399888242023\t0.707221181524575\t0.06188010450000001\n",
      "Experiment: Flow betweenness centrality:HL\n",
      "0.05600\t0.3631547778\t89.825\t109.025\t0.41681143174063984\t0.723590660037975\t0.0615167045\n",
      "Experiment: Flow betweenness centrality:LH\n",
      "0.07400\t0.4076821037\t100.85\t118.3\t0.439569907895596\t0.692976264368925\t0.03086653325\n",
      "Experiment: Information centrality:HL\n",
      "0.04400\t0.3786444049\t93.675\t111.1\t0.41603303436373407\t0.7229581178697749\t0.059036518999999996\n",
      "Experiment: Information centrality:LH\n",
      "0.07500\t0.4229191903\t104.6\t118.375\t0.44042325455375125\t0.7057038386466999\t0.0480359405\n",
      "Experiment: Kleinbergs centrality HITS:HL\n",
      "0.03900\t0.3958971763\t97.975\t116.2\t0.4337632029367535\t0.7087221885151249\t0.04588007774999999\n",
      "Experiment: Kleinbergs centrality HITS:LH\n",
      "0.07400\t0.4254289676\t105.2\t118.125\t0.44240115773750066\t0.704539268789525\t0.0467562095\n",
      "Experiment: LAC:HL\n",
      "0.04000\t0.3812871535\t94.35\t111.9\t0.4244823760998483\t0.720359981212275\t0.06263586525\n",
      "Experiment: LAC:LH\n",
      "0.08200\t0.4201139024\t103.925\t117.2\t0.42961773209157916\t0.707716235820525\t0.040427541\n",
      "Experiment: Lapacian centrality:HL\n",
      "0.04200\t0.3793028974\t93.85\t110.75\t0.41752940332862554\t0.722289791396975\t0.057015158749999996\n",
      "Experiment: Lapacian centrality:LH\n",
      "0.08400\t0.4208051542\t104.1\t119.125\t0.43497036644994774\t0.7021655183895501\t0.04699808325\n",
      "Experiment: Leverage centrality:HL\n",
      "0.05500\t0.3587330210\t88.775\t107.45\t0.4100435734512623\t0.72666553376395\t0.05426210125000001\n",
      "Experiment: Leverage centrality:LH\n",
      "0.08100\t0.4130893185\t102.175\t118.95\t0.43140023043919395\t0.68693035612415\t0.03736867425\n",
      "Experiment: Lin centrality:HL\n",
      "0.03900\t0.3889134399\t96.2\t113.875\t0.42889015493779115\t0.7136833561709249\t0.0595434375\n",
      "Experiment: Lin centrality:LH\n",
      "0.07900\t0.4188194888\t103.65\t117.925\t0.43555391888629835\t0.704898162380125\t0.050251043249999995\n",
      "Experiment: Load centrality:HL\n",
      "0.05600\t0.3618521396\t89.475\t109.075\t0.4214607752940493\t0.722007430246975\t0.0470258575\n",
      "Experiment: Load centrality:LH\n",
      "0.07300\t0.4173175712\t103.2\t118.2\t0.4385864940250886\t0.6971665729462749\t0.03252610375\n",
      "Experiment: Lobby index:HL\n",
      "0.05200\t0.3751283044\t92.8\t110.475\t0.414595623517431\t0.7266093722252499\t0.06740015175\n",
      "Experiment: Lobby index:LH\n",
      "0.08900\t0.4252530277\t105.2\t119.275\t0.4460397354468661\t0.700395867435275\t0.0559311585\n",
      "Experiment: Local assortativity:HL\n",
      "0.06600\t0.3897511987\t96.4\t114.15\t0.43464116336427744\t0.695784637781925\t0.03724399700000001\n",
      "Experiment: Local assortativity:LH\n",
      "0.04100\t0.3901487328\t96.5\t114.075\t0.4216133644763499\t0.706528565563875\t0.05055261725\n",
      "Experiment: Local clustering coefficients:HL\n",
      "0.03700\t0.3964583076\t98.1\t114.075\t0.4288493962815778\t0.719858652752025\t0.0543995435\n",
      "Experiment: Local clustering coefficients:LH\n",
      "0.04900\t0.4007669561\t99.175\t115.425\t0.42322118205507203\t0.7091252480713\t0.040621783\n",
      "Experiment: Markov centrality:HL\n",
      "0.04400\t0.3815179331\t94.375\t111.5\t0.41880739272423595\t0.7211104631742\t0.057057289\n",
      "Experiment: Markov centrality:LH\n",
      "0.08900\t0.4124093532\t102.0\t116.7\t0.42932673960273726\t0.70880836353445\t0.037159522\n",
      "Experiment: MCC centrality:HL\n",
      "0.04700\t0.3739372883\t92.5\t110.1\t0.4142431433921713\t0.7284582542754\t0.05210019075000001\n",
      "Experiment: MCC centrality:LH\n",
      "0.08600\t0.4239425050\t104.85\t118.15\t0.43968003151713403\t0.704552312292025\t0.0404888535\n",
      "Experiment: MNC centrality:HL\n",
      "0.04000\t0.3783346079\t93.625\t110.875\t0.4110095739847412\t0.7232366320429\t0.063594596\n",
      "Experiment: MNC centrality:LH\n",
      "0.08300\t0.4194701406\t103.8\t117.1\t0.4379243489874141\t0.7060860515300249\t0.04181392874999999\n",
      "Experiment: Neighborhood connectivity:HL\n",
      "0.03900\t0.4068817667\t100.65\t117.6\t0.4283136789664116\t0.701876680919325\t0.022631766749999997\n",
      "Experiment: Neighborhood connectivity:LH\n",
      "0.07500\t0.4104791799\t101.55\t116.025\t0.43405747555550683\t0.70501734052425\t0.0440306455\n",
      "Experiment: Network centrality:HL\n",
      "0.04300\t0.3712561927\t91.875\t111.275\t0.4064372249658364\t0.7191778417808751\t0.06712562225\n",
      "Experiment: Network centrality:LH\n",
      "0.07800\t0.4131127912\t102.2\t115.875\t0.4381378434656952\t0.7100363917363001\t0.04903720525\n",
      "Experiment: Network fragmentation GeodesicDistanceWeighted:HL\n",
      "0.04100\t0.3794900597\t93.9\t112.775\t0.41948082609439596\t0.7126207219696751\t0.035512265\n",
      "Experiment: Network fragmentation GeodesicDistanceWeighted:LH\n",
      "0.07400\t0.4114286789\t101.8\t118.375\t0.4330346114793279\t0.7036196282699\t0.03947918975\n",
      "Experiment: Network fragmentation:HL\n",
      "0.04300\t0.3922872065\t97.05\t113.95\t0.43202834774971766\t0.7144370335975501\t0.043320367250000005\n",
      "Experiment: Network fragmentation:LH\n",
      "0.06500\t0.4102516585\t101.5\t117.45\t0.4331123064558714\t0.7048991513359499\t0.044882330250000005\n",
      "Experiment: Path centrality:HL\n",
      "0.03600\t0.3860936111\t95.525\t114.225\t0.4178912818011108\t0.713628554508075\t0.04554068825\n",
      "Experiment: Path centrality:LH\n",
      "0.08400\t0.4122875238\t101.95\t116.225\t0.4283402603827017\t0.708782327354925\t0.05592377024999999\n",
      "Experiment: Political independence index:HL\n",
      "0.05000\t0.3850845557\t95.3\t113.9\t0.4297911761887415\t0.7086391486915999\t0.048286346\n",
      "Experiment: Political independence index:LH\n",
      "0.09300\t0.3767979119\t93.25\t115.1\t0.4366836259851401\t0.68960905244865\t0.04686178875\n",
      "Experiment: Radiality centrality:HL\n",
      "0.03900\t0.3889134399\t96.2\t113.875\t0.42889015493779115\t0.7136833561709249\t0.0595434375\n",
      "Experiment: Radiality centrality:LH\n",
      "0.07900\t0.4188194888\t103.65\t117.925\t0.43555391888629835\t0.704898162380125\t0.050251043249999995\n",
      "Experiment: Random walk betweenness:HL\n",
      "0.04400\t0.3737625556\t92.45\t111.0\t0.40536039151033554\t0.72568228343365\t0.06790964925000001\n",
      "Experiment: Random walk betweenness:LH\n",
      "0.07400\t0.4281944637\t105.85\t120.875\t0.43121555698124536\t0.69462635064555\t0.05486306825\n",
      "Experiment: Random walk closeness:HL\n",
      "0.04400\t0.3815179331\t94.375\t111.5\t0.41926611988239604\t0.7211104631742\t0.0563682235\n",
      "Experiment: Random walk closeness:LH\n",
      "0.08900\t0.4124093532\t102.0\t116.7\t0.4294337550605256\t0.70880836353445\t0.0372299845\n",
      "Experiment: SALSA:HL\n",
      "0.04200\t0.4004555074\t99.075\t119.4\t0.4311405973520534\t0.6963777272081749\t0.043375024000000005\n",
      "Experiment: SALSA:LH\n",
      "0.09000\t0.4234131886\t104.75\t120.2\t0.43262079814654364\t0.6968985005064751\t0.0457656845\n",
      "Experiment: Semi local centrality:HL\n",
      "0.04300\t0.3867063310\t95.725\t114.275\t0.4250523091439158\t0.7120215562697749\t0.05370529225\n",
      "Experiment: Semi local centrality:LH\n",
      "0.09100\t0.4223505830\t104.5\t117.9\t0.4483653378505691\t0.7096842925643501\t0.06181261125\n",
      "Experiment: Shortest path betweenness:HL\n",
      "0.05100\t0.3572308057\t88.35\t107.0\t0.4265100277584617\t0.7219089074323\t0.053563267000000005\n",
      "Experiment: Shortest path betweenness:LH\n",
      "0.08600\t0.4162378378\t102.975\t117.875\t0.421505296592643\t0.6972999528562751\t0.0452965815\n",
      "Experiment: Shortest path closeness:HL\n",
      "0.03900\t0.3889134399\t96.2\t113.875\t0.42889015493779115\t0.7136833561709249\t0.0595434375\n",
      "Experiment: Shortest path closeness:LH\n",
      "0.07900\t0.4188194888\t103.65\t117.925\t0.43555391888629835\t0.704898162380125\t0.050251043249999995\n",
      "Experiment: Shortest path degree:HL\n",
      "0.04900\t0.3691095561\t91.35\t109.925\t0.41095784669826363\t0.7278331069485\t0.07281501375\n",
      "Experiment: Shortest path degree:LH\n",
      "0.07900\t0.4266983415\t105.6\t120.025\t0.4463369305439291\t0.6962742738898001\t0.04190105725\n",
      "Experiment: Strength weighted vertex degree:HL\n",
      "0.04900\t0.3691095561\t91.35\t109.925\t0.41095784669826363\t0.7278331069485\t0.07281501375\n",
      "Experiment: Strength weighted vertex degree:LH\n",
      "0.07900\t0.4266983415\t105.6\t120.025\t0.4463369305439291\t0.6962742738898001\t0.04190105725\n",
      "Experiment: Stress centrality:HL\n",
      "0.04900\t0.3614702385\t89.4\t108.15\t0.415656684674944\t0.7236996880576501\t0.05358126175\n",
      "Experiment: Stress centrality:LH\n",
      "0.09200\t0.4109719554\t101.625\t118.325\t0.4209697171033412\t0.6935998587782499\t0.043080468\n",
      "Experiment: Subgraph:HL\n",
      "0.04300\t0.3892905319\t96.35\t114.7\t0.4227980046726646\t0.71319125846665\t0.05138197950000001\n",
      "Experiment: Subgraph:LH\n",
      "0.08300\t0.4218703520\t104.4\t117.45\t0.44407482857784675\t0.707830126293775\t0.05238743275\n",
      "Experiment: Topological coefficient:HL\n",
      "0.07600\t0.4219542164\t104.375\t120.05\t0.4314628589267249\t0.6999148106341\t0.04337488\n",
      "Experiment: Topological coefficient:LH\n",
      "0.04600\t0.3757280900\t92.975\t112.0\t0.4261511006786172\t0.7252806724437\t0.07005165399999999\n",
      "WASTE metric\n",
      "   average = 0.062147826087\n",
      "   min, max = 0.036 0.093\n",
      "   Path centrality:HL || Political independence index:LH\n",
      "CUT RATIO metric\n",
      "   average = 0.398008279677\n",
      "   min, max = 0.355955940895 0.430745274712\n",
      "   BottleNeck centrality:HL || Cross clique centrality:LH\n",
      "EDGES CUT metric\n",
      "   average = 98.4665217391\n",
      "   min, max = 88.075 106.55\n",
      "   BottleNeck centrality:HL || Cross clique centrality:LH\n",
      "TOTAL COMM VOLUME metric\n",
      "   average = 115.050869565\n",
      "   min, max = 105.525 120.875\n",
      "   BottleNeck centrality:HL || Random walk betweenness:LH\n",
      "Qds metric\n",
      "   average = 0.428215376552\n",
      "   min, max = 0.40536039151 0.452672486471\n",
      "   Random walk betweenness:HL || Alpha:HL\n",
      "LONELINESS metric\n",
      "   average = 0.709651799621\n",
      "   min, max = 0.676137986879 0.72939424167\n",
      "   Alpha:LH || Effectiveness centrality:HL\n",
      "MAXPERM metric\n",
      "   average = 0.0505435901174\n",
      "   min, max = 0.0038875375 0.07281501375\n",
      "   Alpha:LH || Degree centrality:HL\n"
     ]
    }
   ],
   "source": [
    "# analyse the results\n",
    "\n",
    "# find min/max for each score\n",
    "metrics = [\"WASTE\", \"CUT RATIO\", \"EDGES CUT\", \"TOTAL COMM VOLUME\", \"Qds\", \"LONELINESS\", \"MAXPERM\"]\n",
    "\n",
    "max_metric_centrality=[]\n",
    "min_metric_centrality=[]\n",
    "max_metric = []\n",
    "min_metric = []\n",
    "avg_metric = []\n",
    "#metric_sort_dataset = []\n",
    "for metric in metrics:\n",
    "    max_metric_centrality.append(\"\")\n",
    "    min_metric_centrality.append(\"\")\n",
    "    max_metric.append(0.0)\n",
    "    min_metric.append(10000000.0)\n",
    "    avg_metric.append(0.0)\n",
    "    metric_sort_dataset = {}\n",
    "\n",
    "avg_results = {}    \n",
    "for key in list(centralities.keys()):\n",
    "    centrality = centralities[key]\n",
    "    centralityCode = centrality.centralityType + \":\" + centrality.orderType\n",
    "    print(\"Experiment:\", centralityCode)\n",
    "\n",
    "    centrality.loadScores()\n",
    "    centrality.computeStatsScore()\n",
    "    avg_results[centralityCode] = centrality.avgScores\n",
    "\n",
    "    centrality.printScoreline(centrality.avgScores)\n",
    "\n",
    "    for i, metric in enumerate(centrality.avgScores):\n",
    "        if(max_metric[i] < metric):\n",
    "            max_metric[i] = metric\n",
    "            max_metric_centrality[i] = centralityCode\n",
    "        if(min_metric[i] > metric):\n",
    "            min_metric[i] = metric\n",
    "            min_metric_centrality[i] = centralityCode\n",
    "        avg_metric[i] = avg_metric[i] + metric\n",
    "        # index the score\n",
    "        #if metric in metric_sort_dataset[i]:\n",
    "        #    metric_sort_dataset[i][metric].append()\n",
    "\n",
    "with open(os.path.join(parametrized_config['OUTPUT_DIRECTORY'], \"centrality_scores.csv\"), 'w+') as f:\n",
    "    s = \"centrality\"\n",
    "    for metric in metrics:\n",
    "        s = s + \",\" + metric\n",
    "    f.write(s + \"\\n\")\n",
    "    \n",
    "    for key in list(avg_results.keys()):\n",
    "        line = key\n",
    "        for score in avg_results[key]:\n",
    "            line = line + \",\" + str(score)\n",
    "        f.write(line + \"\\n\")\n",
    "        \n",
    "for i, avg in enumerate(avg_metric):\n",
    "    avg_metric[i] = avg / len(centralities)\n",
    "            \n",
    "for i, metric in enumerate(metrics):\n",
    "    print(metric, \"metric\")\n",
    "    print(\"   average =\", avg_metric[i])\n",
    "    print(\"   min, max =\", min_metric[i], max_metric[i])\n",
    "    print(\"  \", min_metric_centrality[i], \"||\", max_metric_centrality[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y <- c(90.0,104.0,87.0,87.0,100.0,101.0,84.0,95.0,84.0,85.0,92.0,94.0,87.0,104.0,87.0,91.0,88.0,82.0,92.0,91.0,98.0,99.0,84.0,99.0,100.0,94.0,93.0,75.0,106.0,95.0,96.0,112.0,106.0,96.0,106.0,83.0,100.0,84.0,85.0,85.0,98.0,95.0,99.0,103.0,125.0,119.0,95.0,102.0,96.0,77.0,99.0,99.0,97.0,99.0,86.0,93.0,105.0,97.0,110.0,82.0,98.0,104.0,98.0,101.0,101.0,107.0,92.0,107.0,105.0,97.0,86.0,96.0,95.0,100.0,97.0,91.0,100.0,102.0,92.0,106.0,91.0,79.0,95.0,78.0,93.0,96.0,84.0,91.0,99.0,77.0,96.0,88.0,82.0,96.0,88.0,89.0,92.0,98.0,104.0,84.0,81.0,102.0,79.0,102.0,88.0,98.0,89.0,96.0,102.0,98.0,87.0,94.0,94.0,89.0,100.0,83.0,91.0,89.0,82.0,86.0,106.0,92.0,109.0,111.0,116.0,111.0,102.0,106.0,103.0,84.0,84.0,102.0,94.0,109.0,98.0,98.0,116.0,103.0,106.0,97.0,108.0,99.0,82.0,116.0,111.0,106.0,114.0,97.0,106.0,104.0,110.0,109.0,103.0,101.0,110.0,97.0,110.0,104.0,101.0,109.0,94.0,90.0,105.0,90.0,109.0,100.0,95.0,90.0,107.0,74.0,90.0,95.0,94.0,97.0,76.0,90.0,105.0,90.0,88.0,99.0,97.0,89.0,86.0,100.0,105.0,99.0,98.0,96.0,107.0,97.0,107.0,99.0,92.0,98.0,99.0,92.0,106.0,98.0,85.0,95.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,89.0,87.0,82.0,79.0,100.0,98.0,87.0,89.0,97.0,70.0,82.0,85.0,80.0,93.0,79.0,90.0,94.0,88.0,90.0,90.0,87.0,93.0,84.0,88.0,93.0,95.0,86.0,77.0,98.0,95.0,93.0,101.0,89.0,85.0,92.0,90.0,93.0,80.0,83.0,83.0,94.0,100.0,110.0,104.0,119.0,107.0,95.0,103.0,103.0,94.0,90.0,99.0,108.0,102.0,93.0,101.0,102.0,98.0,106.0,99.0,90.0,98.0,88.0,104.0,96.0,115.0,112.0,101.0,113.0,88.0,117.0,105.0,107.0,114.0,111.0,106.0,115.0,106.0,95.0,111.0,82.0,78.0,91.0,79.0,101.0,98.0,78.0,85.0,92.0,68.0,84.0,92.0,79.0,94.0,76.0,79.0,97.0,72.0,92.0,89.0,93.0,86.0,90.0,97.0,98.0,90.0,85.0,75.0,100.0,96.0,95.0,93.0,77.0,88.0,96.0,87.0,101.0,91.0,87.0,92.0,106.0,92.0,109.0,95.0,119.0,105.0,99.0,108.0,117.0,83.0,107.0,110.0,90.0,100.0,79.0,90.0,99.0,97.0,101.0,106.0,106.0,104.0,98.0,110.0,95.0,101.0,118.0,105.0,113.0,106.0,108.0,109.0,96.0,104.0,112.0,99.0,114.0,113.0,94.0,109.0,108.0,86.0,98.0,95.0,115.0,100.0,86.0,109.0,105.0,81.0,87.0,98.0,97.0,99.0,93.0,102.0,105.0,95.0,109.0,94.0,99.0,102.0,89.0,101.0,105.0,102.0,101.0,87.0,116.0,105.0,91.0,101.0,86.0,94.0,102.0,99.0,110.0,99.0,88.0,102.0,89.0,87.0,97.0,95.0,100.0,108.0,92.0,89.0,111.0,78.0,92.0,92.0,89.0,98.0,75.0,94.0,101.0,92.0,104.0,93.0,89.0,99.0,86.0,95.0,106.0,103.0,91.0,86.0,108.0,95.0,103.0,110.0,92.0,101.0,85.0,99.0,107.0,99.0,86.0,93.0,92.0,83.0,98.0,92.0,109.0,101.0,92.0,93.0,96.0,79.0,89.0,91.0,87.0,88.0,72.0,91.0,98.0,93.0,94.0,100.0,100.0,89.0,89.0,92.0,105.0,95.0,98.0,93.0,104.0,88.0,96.0,101.0,82.0,95.0,91.0,89.0,97.0,90.0,88.0,94.0,95.0,97.0,105.0,85.0,128.0,113.0,97.0,107.0,100.0,103.0,100.0,102.0,105.0,106.0,99.0,101.0,105.0,98.0,106.0,100.0,101.0,105.0,97.0,104.0,107.0,109.0,96.0,102.0,110.0,96.0,98.0,118.0,114.0,104.0,113.0,98.0,111.0,110.0,98.0,103.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,94.0,84.0,102.0,86.0,107.0,101.0,93.0,87.0,99.0,77.0,90.0,96.0,91.0,100.0,76.0,90.0,109.0,91.0,96.0,98.0,96.0,97.0,88.0,103.0,107.0,93.0,103.0,92.0,104.0,97.0,98.0,105.0,84.0,95.0,96.0,87.0,105.0,95.0,91.0,100.0,105.0,102.0,116.0,106.0,127.0,112.0,99.0,105.0,107.0,85.0,101.0,100.0,96.0,102.0,78.0,88.0,102.0,100.0,111.0,103.0,108.0,108.0,90.0,117.0,112.0,106.0,120.0,89.0,111.0,105.0,113.0,103.0,104.0,105.0,113.0,97.0,107.0,106.0,99.0,104.0,97.0,90.0,97.0,94.0,116.0,107.0,91.0,97.0,101.0,80.0,87.0,92.0,95.0,98.0,93.0,97.0,99.0,93.0,101.0,97.0,96.0,102.0,81.0,99.0,100.0,94.0,104.0,108.0,105.0,96.0,100.0,102.0,85.0,93.0,96.0,87.0,98.0,93.0,82.0,96.0,98.0,94.0,99.0,97.0,126.0,111.0,105.0,108.0,107.0,103.0,96.0,101.0,94.0,99.0,99.0,91.0,93.0,98.0,107.0,99.0,100.0,97.0,85.0,103.0,107.0,102.0,109.0,96.0,104.0,110.0,106.0,107.0,96.0,110.0,112.0,97.0,104.0,104.0,88.0,111.0,84.0,80.0,104.0,82.0,109.0,102.0,85.0,94.0,98.0,72.0,89.0,97.0,85.0,94.0,78.0,91.0,99.0,87.0,92.0,94.0,94.0,89.0,86.0,102.0,94.0,87.0,97.0,93.0,92.0,92.0,98.0,102.0,83.0,93.0,93.0,81.0,103.0,90.0,91.0,90.0,100.0,105.0,109.0,100.0,107.0,100.0,96.0,113.0,103.0,108.0,100.0,110.0,93.0,109.0,92.0,89.0,112.0,96.0,115.0,102.0,106.0,111.0,88.0,108.0,114.0,104.0,115.0,113.0,107.0,103.0,105.0,111.0,93.0,103.0,109.0,93.0,106.0,99.0,92.0,106.0,81.0,79.0,95.0,79.0,104.0,96.0,92.0,91.0,97.0,89.0,85.0,97.0,84.0,97.0,76.0,80.0,104.0,86.0,96.0,90.0,96.0,90.0,73.0,94.0,109.0,98.0,93.0,83.0,101.0,83.0,89.0,99.0,93.0,93.0,94.0,84.0,98.0,96.0,80.0,90.0,90.0,100.0,109.0,107.0,125.0,108.0,103.0,104.0,105.0,98.0,103.0,101.0,92.0,103.0,97.0,107.0,105.0,96.0,105.0,103.0,111.0,102.0,95.0,101.0,107.0,109.0,113.0,97.0,110.0,106.0,106.0,112.0,95.0,106.0,104.0,94.0,112.0,116.0,90.0,104.0,101.0,87.0,102.0,93.0,109.0,95.0,89.0,100.0,91.0,93.0,96.0,99.0,88.0,95.0,93.0,95.0,95.0,102.0,100.0,102.0,93.0,98.0,88.0,103.0,106.0,96.0,105.0,89.0,103.0,105.0,111.0,100.0,88.0,97.0,102.0,92.0,105.0,96.0,83.0,88.0,106.0,109.0,93.0,98.0,117.0,116.0,104.0,107.0,113.0,93.0,102.0,99.0,104.0,98.0,97.0,107.0,106.0,102.0,110.0,100.0,104.0,94.0,91.0,121.0,118.0,106.0,117.0,101.0,115.0,96.0,119.0,115.0,100.0,109.0,111.0,101.0,101.0,100.0,99.0,110.0,101.0,102.0,107.0,105.0,130.0,107.0,103.0,108.0,106.0,105.0,103.0,99.0,96.0,113.0,104.0,106.0,114.0,103.0,102.0,108.0,110.0,99.0,99.0,111.0,113.0,116.0,110.0,108.0,112.0,108.0,93.0,115.0,93.0,106.0,114.0,101.0,114.0,108.0,101.0,109.0,86.0,83.0,101.0,91.0,106.0,100.0,94.0,95.0,90.0,86.0,92.0,95.0,90.0,103.0,67.0,92.0,104.0,86.0,94.0,98.0,96.0,87.0,80.0,96.0,99.0,93.0,98.0,92.0,92.0,95.0,101.0,96.0,98.0,89.0,98.0,90.0,98.0,100.0,92.0,86.0,84.0,88.0,96.0,83.0,106.0,105.0,90.0,90.0,103.0,80.0,87.0,91.0,86.0,97.0,74.0,92.0,103.0,93.0,91.0,99.0,98.0,90.0,85.0,101.0,103.0,94.0,102.0,90.0,99.0,87.0,101.0,103.0,81.0,103.0,96.0,92.0,97.0,98.0,95.0,94.0,99.0,102.0,107.0,98.0,117.0,111.0,100.0,110.0,110.0,103.0,108.0,107.0,93.0,105.0,98.0,103.0,105.0,93.0,104.0,93.0,102.0,106.0,91.0,117.0,117.0,105.0,119.0,104.0,104.0,97.0,102.0,117.0,105.0,102.0,94.0,111.0,109.0,105.0,104.0,107.0,97.0,84.0,104.0,88.0,112.0,100.0,93.0,90.0,98.0,84.0,88.0,92.0,88.0,90.0,72.0,97.0,107.0,90.0,97.0,98.0,97.0,90.0,85.0,103.0,109.0,103.0,101.0,98.0,102.0,98.0,98.0,103.0,90.0,95.0,91.0,94.0,105.0,101.0,93.0,99.0,97.0,103.0,93.0,106.0,121.0,111.0,102.0,102.0,104.0,79.0,97.0,104.0,99.0,106.0,86.0,103.0,119.0,108.0,96.0,105.0,106.0,106.0,95.0,112.0,106.0,110.0,103.0,110.0,108.0,109.0,108.0,108.0,99.0,111.0,100.0,106.0,113.0,110.0,91.0,106.0,97.0,84.0,104.0,88.0,112.0,100.0,93.0,90.0,98.0,84.0,88.0,92.0,88.0,90.0,72.0,97.0,107.0,90.0,97.0,98.0,97.0,90.0,85.0,103.0,109.0,103.0,101.0,98.0,102.0,98.0,98.0,103.0,90.0,95.0,91.0,94.0,105.0,101.0,93.0,99.0,97.0,103.0,93.0,106.0,121.0,111.0,102.0,102.0,104.0,79.0,97.0,104.0,99.0,106.0,86.0,103.0,119.0,108.0,96.0,105.0,106.0,106.0,95.0,112.0,106.0,110.0,103.0,110.0,108.0,109.0,108.0,108.0,99.0,111.0,100.0,106.0,113.0,110.0,91.0,106.0,89.0,82.0,96.0,84.0,107.0,97.0,88.0,90.0,94.0,77.0,92.0,90.0,86.0,91.0,68.0,90.0,109.0,90.0,92.0,93.0,96.0,88.0,79.0,100.0,102.0,96.0,94.0,94.0,98.0,87.0,94.0,102.0,84.0,90.0,97.0,84.0,99.0,92.0,88.0,85.0,94.0,107.0,108.0,106.0,116.0,117.0,99.0,105.0,102.0,93.0,110.0,107.0,100.0,111.0,80.0,101.0,115.0,101.0,109.0,103.0,108.0,98.0,89.0,120.0,125.0,111.0,122.0,107.0,109.0,105.0,111.0,114.0,91.0,108.0,98.0,108.0,107.0,105.0,90.0,114.0,90.0,91.0,109.0,92.0,107.0,98.0,97.0,93.0,104.0,84.0,87.0,97.0,91.0,95.0,72.0,91.0,104.0,88.0,96.0,95.0,102.0,89.0,82.0,103.0,106.0,92.0,100.0,93.0,106.0,93.0,94.0,99.0,93.0,101.0,94.0,86.0,106.0,96.0,89.0,96.0,96.0,108.0,101.0,115.0,106.0,102.0,109.0,95.0,109.0,96.0,103.0,105.0,102.0,106.0,84.0,103.0,111.0,109.0,107.0,104.0,108.0,106.0,90.0,110.0,103.0,116.0,113.0,96.0,109.0,103.0,109.0,115.0,93.0,112.0,102.0,97.0,105.0,104.0,95.0,114.0,87.0,95.0,100.0,89.0,119.0,115.0,95.0,92.0,99.0,77.0,98.0,97.0,98.0,111.0,92.0,93.0,102.0,102.0,98.0,99.0,99.0,95.0,92.0,105.0,99.0,104.0,105.0,100.0,100.0,101.0,102.0,102.0,92.0,103.0,97.0,88.0,110.0,99.0,93.0,102.0,98.0,81.0,97.0,101.0,118.0,107.0,104.0,102.0,95.0,80.0,103.0,112.0,98.0,97.0,88.0,101.0,107.0,95.0,101.0,99.0,98.0,100.0,89.0,102.0,108.0,107.0,113.0,101.0,108.0,104.0,102.0,109.0,88.0,95.0,112.0,94.0,109.0,112.0,99.0,100.0,110.0,90.0,95.0,105.0,120.0,112.0,103.0,107.0,100.0,101.0,103.0,109.0,96.0,100.0,96.0,89.0,95.0,93.0,98.0,89.0,100.0,105.0,90.0,102.0,110.0,109.0,109.0,111.0,98.0,96.0,97.0,100.0,94.0,107.0,114.0,91.0,101.0,108.0,90.0,99.0,105.0,84.0,102.0,88.0,111.0,109.0,85.0,97.0,97.0,79.0,92.0,97.0,96.0,102.0,95.0,93.0,96.0,89.0,102.0,95.0,96.0,92.0,82.0,96.0,98.0,97.0,99.0,89.0,101.0,107.0,100.0,109.0,94.0,97.0,91.0,91.0,109.0,96.0,96.0,89.0,81.0,75.0,95.0,87.0,100.0,96.0,82.0,89.0,92.0,77.0,81.0,95.0,80.0,94.0,67.0,80.0,99.0,87.0,86.0,90.0,89.0,87.0,80.0,95.0,97.0,95.0,85.0,79.0,96.0,86.0,98.0,105.0,91.0,92.0,95.0,90.0,97.0,91.0,91.0,85.0,96.0,97.0,106.0,97.0,117.0,116.0,103.0,107.0,104.0,100.0,108.0,102.0,95.0,121.0,96.0,105.0,113.0,101.0,108.0,101.0,100.0,100.0,95.0,97.0,111.0,110.0,107.0,98.0,114.0,105.0,92.0,116.0,100.0,106.0,99.0,102.0,100.0,92.0,93.0,107.0,88.0,95.0,103.0,96.0,112.0,106.0,94.0,98.0,103.0,86.0,99.0,102.0,97.0,91.0,87.0,91.0,102.0,99.0,107.0,94.0,99.0,92.0,84.0,112.0,113.0,97.0,99.0,96.0,105.0,94.0,100.0,102.0,92.0,101.0,99.0,91.0,107.0,104.0,89.0,93.0,107.0,101.0,110.0,101.0,113.0,110.0,96.0,110.0,111.0,84.0,95.0,102.0,111.0,102.0,83.0,97.0,104.0,109.0,114.0,101.0,107.0,108.0,98.0,106.0,110.0,118.0,110.0,112.0,115.0,107.0,107.0,106.0,91.0,109.0,107.0,104.0,113.0,110.0,102.0,107.0,92.0,91.0,99.0,96.0,123.0,99.0,98.0,99.0,105.0,92.0,100.0,108.0,96.0,99.0,99.0,96.0,107.0,101.0,105.0,95.0,100.0,100.0,92.0,104.0,107.0,108.0,102.0,98.0,111.0,112.0,113.0,101.0,86.0,104.0,107.0,99.0,106.0,89.0,95.0,105.0,94.0,87.0,87.0,95.0,114.0,105.0,90.0,103.0,101.0,80.0,94.0,98.0,90.0,97.0,81.0,95.0,94.0,84.0,93.0,90.0,91.0,94.0,89.0,102.0,99.0,95.0,97.0,81.0,98.0,103.0,86.0,105.0,97.0,99.0,106.0,94.0,101.0,97.0,90.0,108.0,90.0,87.0,86.0,90.0,110.0,99.0,91.0,90.0,91.0,79.0,90.0,97.0,82.0,94.0,72.0,88.0,106.0,88.0,99.0,98.0,97.0,91.0,82.0,108.0,98.0,100.0,98.0,96.0,106.0,91.0,103.0,98.0,80.0,87.0,98.0,94.0,98.0,100.0,86.0,102.0,98.0,99.0,108.0,89.0,119.0,107.0,104.0,106.0,110.0,97.0,104.0,104.0,101.0,103.0,97.0,111.0,107.0,105.0,109.0,105.0,110.0,110.0,89.0,116.0,112.0,105.0,109.0,107.0,105.0,99.0,118.0,115.0,96.0,101.0,110.0,99.0,109.0,99.0,102.0,104.0,80.0,93.0,89.0,74.0,98.0,104.0,84.0,90.0,99.0,83.0,87.0,91.0,83.0,95.0,71.0,84.0,96.0,87.0,90.0,92.0,98.0,101.0,85.0,90.0,88.0,91.0,96.0,77.0,95.0,97.0,87.0,92.0,83.0,94.0,99.0,89.0,102.0,84.0,80.0,95.0,101.0,100.0,100.0,94.0,118.0,111.0,100.0,102.0,99.0,95.0,101.0,107.0,103.0,109.0,96.0,99.0,102.0,104.0,102.0,95.0,99.0,95.0,84.0,107.0,102.0,97.0,101.0,95.0,123.0,89.0,95.0,115.0,94.0,111.0,93.0,102.0,106.0,92.0,92.0,104.0,84.0,88.0,96.0,83.0,106.0,105.0,90.0,90.0,103.0,80.0,87.0,91.0,86.0,97.0,74.0,92.0,103.0,93.0,91.0,99.0,98.0,90.0,85.0,101.0,103.0,94.0,102.0,90.0,99.0,87.0,101.0,103.0,81.0,103.0,96.0,92.0,97.0,98.0,95.0,94.0,99.0,102.0,107.0,98.0,117.0,111.0,100.0,110.0,110.0,103.0,108.0,107.0,93.0,105.0,98.0,103.0,105.0,93.0,104.0,93.0,102.0,106.0,91.0,117.0,117.0,105.0,119.0,104.0,104.0,97.0,102.0,117.0,105.0,102.0,94.0,111.0,109.0,105.0,104.0,107.0,88.0,95.0,103.0,96.0,112.0,106.0,94.0,98.0,103.0,86.0,99.0,102.0,97.0,91.0,87.0,91.0,102.0,99.0,107.0,94.0,99.0,92.0,84.0,112.0,113.0,97.0,99.0,96.0,105.0,94.0,100.0,102.0,92.0,101.0,99.0,91.0,107.0,104.0,89.0,93.0,107.0,101.0,110.0,101.0,113.0,110.0,96.0,110.0,111.0,84.0,95.0,102.0,111.0,102.0,83.0,97.0,104.0,109.0,114.0,101.0,107.0,108.0,98.0,106.0,110.0,118.0,110.0,112.0,115.0,107.0,107.0,106.0,91.0,109.0,107.0,104.0,113.0,110.0,102.0,107.0,82.0,85.0,102.0,94.0,111.0,100.0,92.0,95.0,102.0,84.0,90.0,91.0,88.0,93.0,87.0,88.0,99.0,93.0,96.0,91.0,101.0,92.0,81.0,99.0,106.0,97.0,103.0,90.0,97.0,95.0,95.0,103.0,81.0,99.0,95.0,91.0,105.0,99.0,88.0,94.0,90.0,102.0,106.0,102.0,118.0,110.0,98.0,101.0,96.0,103.0,106.0,111.0,92.0,109.0,103.0,98.0,98.0,102.0,111.0,92.0,99.0,104.0,97.0,112.0,107.0,98.0,119.0,99.0,116.0,102.0,117.0,110.0,98.0,102.0,114.0,101.0,113.0,96.0,98.0,107.0,89.0,86.0,102.0,89.0,111.0,107.0,94.0,92.0,98.0,83.0,93.0,96.0,89.0,91.0,77.0,86.0,110.0,86.0,95.0,94.0,100.0,88.0,84.0,97.0,104.0,90.0,98.0,97.0,101.0,94.0,99.0,97.0,94.0,95.0,87.0,89.0,103.0,92.0,89.0,88.0,97.0,100.0,109.0,91.0,117.0,109.0,111.0,108.0,111.0,87.0,97.0,103.0,90.0,107.0,95.0,106.0,113.0,100.0,114.0,104.0,111.0,99.0,92.0,103.0,117.0,97.0,110.0,104.0,114.0,108.0,107.0,111.0,93.0,103.0,108.0,104.0,102.0,102.0,98.0,112.0,83.0,87.0,95.0,76.0,106.0,96.0,78.0,88.0,92.0,65.0,85.0,91.0,84.0,95.0,64.0,85.0,93.0,87.0,91.0,90.0,86.0,97.0,78.0,93.0,94.0,88.0,91.0,79.0,92.0,82.0,98.0,97.0,86.0,96.0,99.0,93.0,93.0,96.0,88.0,94.0,102.0,91.0,106.0,91.0,120.0,111.0,108.0,111.0,106.0,94.0,98.0,99.0,96.0,105.0,101.0,97.0,101.0,101.0,112.0,97.0,100.0,110.0,91.0,114.0,96.0,104.0,107.0,96.0,104.0,100.0,110.0,106.0,87.0,105.0,106.0,96.0,107.0,104.0,93.0,104.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,92.0,81.0,82.0,74.0,96.0,94.0,88.0,86.0,99.0,71.0,86.0,89.0,84.0,101.0,77.0,94.0,95.0,91.0,89.0,97.0,90.0,97.0,84.0,98.0,95.0,88.0,85.0,79.0,98.0,92.0,98.0,100.0,88.0,85.0,89.0,88.0,97.0,89.0,82.0,91.0,95.0,103.0,109.0,98.0,108.0,109.0,93.0,104.0,107.0,100.0,91.0,112.0,105.0,107.0,96.0,104.0,99.0,105.0,103.0,96.0,101.0,107.0,91.0,107.0,108.0,106.0,114.0,89.0,118.0,102.0,103.0,111.0,107.0,97.0,101.0,98.0,108.0,111.0,98.0,107.0,89.0,81.0,95.0,92.0,99.0,97.0,93.0,95.0,92.0,90.0,92.0,91.0,82.0,96.0,74.0,85.0,93.0,94.0,95.0,95.0,95.0,92.0,81.0,101.0,101.0,93.0,98.0,91.0,114.0,95.0,104.0,97.0,81.0,97.0,92.0,92.0,101.0,88.0,87.0,92.0,106.0,110.0,102.0,106.0,126.0,107.0,99.0,109.0,106.0,96.0,101.0,97.0,98.0,104.0,92.0,96.0,107.0,109.0,109.0,107.0,107.0,111.0,93.0,111.0,107.0,105.0,101.0,109.0,115.0,110.0,106.0,106.0,105.0,110.0,105.0,97.0,109.0,104.0,98.0,112.0,97.0,94.0,93.0,93.0,115.0,105.0,98.0,91.0,99.0,100.0,98.0,88.0,94.0,98.0,86.0,90.0,103.0,89.0,104.0,88.0,88.0,100.0,83.0,119.0,90.0,102.0,100.0,86.0,101.0,96.0,90.0,105.0,89.0,98.0,88.0,92.0,103.0,111.0,94.0,98.0,95.0,87.0,100.0,96.0,108.0,108.0,101.0,98.0,99.0,82.0,109.0,100.0,97.0,99.0,80.0,87.0,91.0,90.0,110.0,91.0,86.0,100.0,91.0,98.0,109.0,104.0,95.0,86.0,104.0,96.0,85.0,105.0,100.0,100.0,97.0,89.0,105.0,100.0,83.0,99.0,94.0,97.0,95.0,93.0,116.0,103.0,97.0,95.0,106.0,98.0,101.0,92.0,91.0,106.0,76.0,96.0,105.0,93.0,94.0,94.0,96.0,104.0,84.0,112.0,105.0,108.0,97.0,98.0,106.0,91.0,103.0,106.0,95.0,102.0,92.0,91.0,107.0,98.0,91.0,96.0,93.0,92.0,96.0,100.0,112.0,108.0,99.0,103.0,99.0,87.0,103.0,103.0,95.0,100.0,84.0,97.0,105.0,95.0,95.0,101.0,92.0,96.0,85.0,114.0,108.0,101.0,101.0,103.0,106.0,100.0,101.0,102.0,99.0,96.0,100.0,91.0,118.0,102.0,87.0,98.0,84.0,88.0,97.0,82.0,104.0,96.0,90.0,95.0,102.0,81.0,91.0,93.0,89.0,96.0,74.0,100.0,102.0,100.0,90.0,99.0,98.0,94.0,87.0,101.0,108.0,94.0,99.0,91.0,97.0,87.0,94.0,103.0,90.0,103.0,104.0,92.0,98.0,98.0,90.0,94.0,96.0,94.0,108.0,99.0,117.0,107.0,100.0,103.0,97.0,102.0,95.0,105.0,93.0,105.0,85.0,104.0,109.0,101.0,107.0,109.0,102.0,106.0,99.0,117.0,112.0,105.0,118.0,99.0,104.0,97.0,98.0,106.0,101.0,100.0,92.0,91.0,111.0,100.0,95.0,91.0,87.0,91.0,99.0,93.0,103.0,102.0,96.0,94.0,94.0,82.0,89.0,99.0,88.0,93.0,70.0,89.0,100.0,93.0,97.0,92.0,94.0,86.0,82.0,97.0,100.0,90.0,96.0,100.0,94.0,92.0,99.0,93.0,84.0,97.0,91.0,90.0,101.0,94.0,83.0,86.0,104.0,85.0,108.0,98.0,118.0,107.0,101.0,107.0,107.0,101.0,102.0,112.0,100.0,103.0,86.0,91.0,112.0,98.0,101.0,108.0,107.0,110.0,92.0,104.0,103.0,110.0,111.0,100.0,112.0,104.0,109.0,106.0,104.0,107.0,109.0,109.0,122.0,110.0,97.0,119.0,90.0,87.0,104.0,90.0,114.0,96.0,89.0,100.0,97.0,86.0,91.0,91.0,88.0,95.0,82.0,95.0,102.0,93.0,98.0,91.0,98.0,92.0,84.0,102.0,101.0,95.0,98.0,91.0,104.0,89.0,101.0,99.0,76.0,96.0,89.0,90.0,102.0,94.0,81.0,84.0,97.0,100.0,103.0,91.0,117.0,115.0,102.0,110.0,98.0,89.0,94.0,105.0,100.0,105.0,96.0,112.0,112.0,91.0,114.0,102.0,100.0,99.0,89.0,109.0,99.0,109.0,110.0,104.0,118.0,110.0,107.0,111.0,94.0,112.0,104.0,101.0,114.0,97.0,93.0,119.0,94.0,99.0,108.0,104.0,119.0,110.0,94.0,107.0,107.0,92.0,101.0,99.0,100.0,101.0,97.0,95.0,108.0,95.0,98.0,88.0,97.0,102.0,92.0,97.0,106.0,106.0,101.0,97.0,109.0,104.0,105.0,102.0,89.0,109.0,101.0,96.0,102.0,110.0,93.0,92.0,94.0,96.0,100.0,96.0,124.0,102.0,107.0,98.0,108.0,88.0,100.0,103.0,98.0,107.0,88.0,90.0,110.0,102.0,95.0,104.0,104.0,92.0,95.0,104.0,107.0,107.0,107.0,101.0,114.0,100.0,108.0,107.0,97.0,102.0,110.0,90.0,107.0,108.0,92.0,100.0,85.0,87.0,96.0,94.0,108.0,103.0,86.0,89.0,99.0,86.0,88.0,95.0,87.0,94.0,67.0,92.0,102.0,89.0,99.0,94.0,89.0,87.0,84.0,98.0,99.0,96.0,97.0,80.0,100.0,91.0,98.0,102.0,83.0,93.0,92.0,84.0,102.0,92.0,84.0,84.0,89.0,100.0,105.0,93.0,126.0,106.0,107.0,111.0,105.0,85.0,94.0,99.0,100.0,104.0,106.0,100.0,108.0,98.0,98.0,97.0,106.0,97.0,96.0,108.0,115.0,105.0,91.0,94.0,114.0,98.0,113.0,106.0,97.0,101.0,108.0,99.0,101.0,106.0,98.0,104.0,93.0,86.0,95.0,81.0,103.0,103.0,91.0,92.0,101.0,84.0,96.0,95.0,82.0,100.0,79.0,94.0,96.0,85.0,94.0,100.0,95.0,103.0,79.0,97.0,108.0,95.0,88.0,86.0,103.0,97.0,95.0,99.0,93.0,102.0,96.0,95.0,103.0,97.0,82.0,93.0,97.0,97.0,108.0,89.0,117.0,114.0,100.0,109.0,104.0,81.0,92.0,112.0,102.0,103.0,87.0,89.0,106.0,101.0,104.0,100.0,99.0,101.0,91.0,107.0,104.0,107.0,114.0,97.0,111.0,97.0,108.0,116.0,109.0,104.0,96.0,102.0,113.0,101.0,88.0,95.0,97.0,89.0,98.0,95.0,106.0,105.0,95.0,105.0,99.0,90.0,81.0,93.0,85.0,94.0,92.0,97.0,97.0,105.0,102.0,90.0,88.0,94.0,81.0,103.0,100.0,99.0,96.0,81.0,100.0,95.0,111.0,109.0,105.0,95.0,102.0,94.0,109.0,105.0,96.0,104.0,102.0,100.0,87.0,102.0,119.0,113.0,102.0,107.0,90.0,97.0,98.0,95.0,91.0,107.0,91.0,92.0,108.0,98.0,110.0,100.0,106.0,106.0,91.0,107.0,101.0,111.0,100.0,94.0,113.0,107.0,98.0,109.0,95.0,100.0,109.0,99.0,107.0,106.0,88.0,104.0,90.0,87.0,105.0,86.0,113.0,100.0,87.0,93.0,106.0,79.0,87.0,92.0,98.0,101.0,90.0,91.0,102.0,79.0,104.0,106.0,98.0,91.0,89.0,102.0,100.0,92.0,102.0,93.0,110.0,93.0,92.0,104.0,83.0,95.0,103.0,93.0,102.0,94.0,90.0,99.0,95.0,96.0,101.0,96.0,110.0,106.0,97.0,100.0,108.0,99.0,105.0,106.0,102.0,103.0,92.0,98.0,97.0,99.0,96.0,103.0,98.0,105.0,97.0,112.0,116.0,106.0,115.0,101.0,118.0,99.0,90.0,116.0,94.0,93.0,107.0,98.0,104.0,108.0,97.0,95.0,86.0,92.0,99.0,79.0,112.0,103.0,84.0,95.0,103.0,74.0,106.0,91.0,89.0,103.0,84.0,97.0,100.0,96.0,85.0,93.0,99.0,95.0,87.0,108.0,91.0,88.0,110.0,92.0,111.0,101.0,96.0,103.0,99.0,101.0,99.0,91.0,105.0,96.0,78.0,91.0,105.0,97.0,89.0,76.0,103.0,101.0,86.0,90.0,99.0,75.0,97.0,101.0,97.0,99.0,91.0,87.0,103.0,79.0,109.0,83.0,90.0,100.0,79.0,115.0,108.0,91.0,86.0,99.0,109.0,90.0,85.0,104.0,83.0,94.0,94.0,83.0,94.0,80.0,90.0,89.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,85.0,85.0,101.0,88.0,104.0,98.0,93.0,91.0,97.0,79.0,90.0,98.0,93.0,95.0,69.0,92.0,109.0,92.0,95.0,94.0,95.0,91.0,82.0,99.0,101.0,89.0,94.0,83.0,98.0,93.0,106.0,97.0,85.0,97.0,84.0,89.0,102.0,91.0,87.0,87.0,98.0,107.0,116.0,90.0,119.0,109.0,109.0,114.0,99.0,107.0,103.0,111.0,104.0,113.0,102.0,100.0,111.0,105.0,98.0,101.0,105.0,102.0,100.0,107.0,99.0,113.0,108.0,100.0,110.0,105.0,114.0,110.0,103.0,114.0,112.0,98.0,102.0,103.0,102.0,111.0,84.0,88.0,97.0,82.0,104.0,96.0,90.0,95.0,102.0,81.0,91.0,93.0,89.0,96.0,74.0,100.0,102.0,100.0,90.0,99.0,98.0,94.0,87.0,101.0,108.0,94.0,99.0,91.0,97.0,87.0,94.0,103.0,90.0,103.0,104.0,92.0,98.0,98.0,90.0,94.0,96.0,94.0,108.0,99.0,117.0,107.0,100.0,103.0,97.0,102.0,95.0,105.0,93.0,105.0,85.0,104.0,109.0,101.0,107.0,109.0,102.0,106.0,99.0,117.0,112.0,105.0,118.0,99.0,104.0,97.0,98.0,106.0,101.0,100.0,92.0,91.0,111.0,100.0,95.0,91.0,100.0,99.0,96.0,91.0,121.0,106.0,91.0,96.0,95.0,101.0,95.0,107.0,95.0,104.0,88.0,98.0,104.0,95.0,96.0,101.0,93.0,107.0,92.0,108.0,104.0,96.0,93.0,99.0,105.0,98.0,97.0,108.0,87.0,107.0,96.0,95.0,105.0,108.0,85.0,101.0,101.0,107.0,104.0,102.0,116.0,116.0,103.0,105.0,100.0,90.0,99.0,107.0,98.0,100.0,97.0,95.0,107.0,101.0,112.0,100.0,108.0,101.0,91.0,109.0,113.0,108.0,123.0,92.0,112.0,104.0,112.0,104.0,101.0,110.0,110.0,104.0,106.0,108.0,95.0,119.0,85.0,86.0,98.0,96.0,114.0,99.0,92.0,92.0,103.0,87.0,93.0,97.0,90.0,94.0,78.0,92.0,106.0,98.0,100.0,95.0,96.0,91.0,79.0,104.0,109.0,93.0,101.0,95.0,110.0,97.0,101.0,107.0,90.0,95.0,99.0,89.0,100.0,99.0,90.0,89.0,96.0,101.0,106.0,100.0,128.0,103.0,96.0,101.0,109.0,85.0,108.0,107.0,100.0,103.0,98.0,108.0,115.0,108.0,102.0,101.0,101.0,103.0,92.0,115.0,118.0,110.0,97.0,114.0,104.0,105.0,114.0,114.0,94.0,102.0,106.0,101.0,111.0,108.0,98.0,98.0,89.0,87.0,82.0,79.0,100.0,98.0,87.0,89.0,97.0,70.0,82.0,85.0,80.0,93.0,79.0,90.0,94.0,88.0,90.0,90.0,87.0,93.0,84.0,88.0,93.0,95.0,86.0,77.0,98.0,95.0,93.0,101.0,89.0,85.0,92.0,90.0,93.0,80.0,83.0,83.0,94.0,100.0,110.0,104.0,119.0,107.0,95.0,103.0,103.0,94.0,90.0,99.0,108.0,102.0,93.0,101.0,102.0,98.0,106.0,99.0,90.0,98.0,88.0,104.0,96.0,115.0,112.0,101.0,113.0,88.0,117.0,105.0,107.0,114.0,111.0,106.0,115.0,106.0,95.0,111.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,89.0,82.0,96.0,84.0,107.0,97.0,88.0,90.0,94.0,77.0,92.0,90.0,86.0,91.0,68.0,90.0,109.0,90.0,92.0,93.0,96.0,88.0,79.0,100.0,102.0,96.0,94.0,94.0,98.0,87.0,94.0,102.0,84.0,90.0,97.0,84.0,99.0,92.0,88.0,85.0,94.0,107.0,108.0,106.0,116.0,117.0,99.0,105.0,102.0,93.0,110.0,107.0,100.0,111.0,80.0,101.0,115.0,101.0,109.0,103.0,108.0,98.0,89.0,120.0,125.0,111.0,122.0,107.0,109.0,105.0,111.0,114.0,91.0,108.0,98.0,108.0,107.0,105.0,90.0,114.0,89.0,82.0,96.0,84.0,107.0,97.0,88.0,90.0,94.0,77.0,92.0,90.0,86.0,91.0,68.0,90.0,109.0,90.0,92.0,93.0,96.0,88.0,79.0,100.0,102.0,96.0,94.0,94.0,98.0,87.0,94.0,102.0,84.0,90.0,97.0,84.0,99.0,92.0,88.0,85.0,94.0,107.0,108.0,106.0,116.0,117.0,99.0,105.0,102.0,93.0,110.0,107.0,100.0,111.0,80.0,101.0,115.0,101.0,109.0,103.0,108.0,98.0,89.0,120.0,125.0,111.0,122.0,107.0,109.0,105.0,111.0,114.0,91.0,108.0,98.0,108.0,107.0,105.0,90.0,114.0,87.0,86.0,97.0,80.0,96.0,89.0,83.0,85.0,88.0,73.0,84.0,91.0,82.0,96.0,90.0,90.0,94.0,86.0,92.0,96.0,82.0,91.0,81.0,86.0,100.0,90.0,86.0,88.0,100.0,97.0,92.0,94.0,83.0,90.0,99.0,88.0,98.0,95.0,84.0,87.0,101.0,90.0,103.0,104.0,113.0,97.0,102.0,109.0,98.0,85.0,98.0,101.0,102.0,100.0,99.0,88.0,104.0,109.0,95.0,99.0,90.0,100.0,91.0,120.0,115.0,111.0,93.0,94.0,111.0,112.0,96.0,113.0,99.0,96.0,97.0,107.0,112.0,111.0,97.0,103.0,88.0,82.0,103.0,99.0,112.0,103.0,95.0,97.0,99.0,87.0,97.0,97.0,95.0,88.0,83.0,91.0,102.0,95.0,96.0,98.0,97.0,90.0,79.0,107.0,104.0,97.0,103.0,97.0,105.0,95.0,101.0,104.0,84.0,111.0,95.0,87.0,109.0,100.0,86.0,96.0,101.0,95.0,108.0,97.0,125.0,105.0,95.0,103.0,106.0,91.0,110.0,103.0,97.0,108.0,99.0,97.0,119.0,96.0,118.0,96.0,102.0,113.0,97.0,109.0,104.0,103.0,110.0,95.0,119.0,108.0,114.0,116.0,94.0,111.0,106.0,90.0,112.0,109.0,91.0,104.0,105.0,102.0,103.0,95.0,117.0,104.0,105.0,105.0,103.0,84.0,96.0,107.0,98.0,105.0,94.0,101.0,111.0,101.0,115.0,108.0,109.0,111.0,90.0,103.0,109.0,102.0,117.0,104.0,112.0,103.0,106.0,106.0,104.0,112.0,110.0,100.0,117.0,106.0,96.0,99.0,84.0,90.0,97.0,75.0,102.0,98.0,87.0,89.0,93.0,76.0,92.0,89.0,87.0,98.0,68.0,92.0,102.0,92.0,94.0,99.0,99.0,91.0,87.0,89.0,110.0,99.0,99.0,81.0,105.0,93.0,102.0,99.0,92.0,96.0,103.0,88.0,107.0,102.0,81.0,92.0)\n",
      "\n",
      "g <- as.factor(c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,51,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114))\n"
     ]
    }
   ],
   "source": [
    "# Extract variables for R analysis\n",
    "y = ''\n",
    "g = ''\n",
    "\n",
    "tmpY = ''\n",
    "tmpG = ''\n",
    "\n",
    "\n",
    "gkey = {}\n",
    "\n",
    "modeScore = {} # list of centralities for each modal edges cut value\n",
    "\n",
    "# this is to set the CONTROL for the R statistical tests (control = first centrality data that has to go in)\n",
    "\n",
    "# not sure - this would probably be the Leverage_Centrality_HL which has technically\n",
    "# the best modal score\n",
    "whichIDToKeepAsZero = 0 #leverage # 90 for Political_independence_index_LH # 0 for random \n",
    "\n",
    "for i_key, key in enumerate(list(centralities.keys())):\n",
    "    if(i_key < whichIDToKeepAsZero):\n",
    "        gkey[i_key + 1] = key\n",
    "    elif(i_key == whichIDToKeepAsZero):\n",
    "        gkey[0] = key\n",
    "    else:\n",
    "        gkey[i_key] = key\n",
    "        \n",
    "    centrality = centralities[key]\n",
    "    \n",
    "    # extract Edges Cut mode value\n",
    "    modescore = centrality.modeScores[2].split(':')\n",
    "    mecut = float(modescore[0]) # value of mode of edges cut\n",
    "    mcount = int(modescore[1]) # number of experiments with this modal value of edges cut\n",
    "    \n",
    "\n",
    "    # store each centrality based on their modal value of edges cut\n",
    "    #overallmodescore = mecut / mcount\n",
    "    overallmodescore = mecut\n",
    "    if overallmodescore in modeScore:\n",
    "        modeScore[overallmodescore].append(key)\n",
    "    else:\n",
    "        modeScore[overallmodescore] = [key]\n",
    "    \n",
    "    for i, score in enumerate(centrality.scores):\n",
    "        edges_cut = score[2]\n",
    "        \n",
    "        if(i_key == whichIDToKeepAsZero):\n",
    "            if(len(tmpY)):\n",
    "                tmpY += ','\n",
    "            tmpY += str(edges_cut)\n",
    "            if(len(tmpG)):\n",
    "                tmpG += ','\n",
    "            tmpG += str(0)\n",
    "            \n",
    "        else:        \n",
    "            if(len(y)):\n",
    "                y += ','\n",
    "            y += str(edges_cut)\n",
    "            if(len(g)):\n",
    "                g += ','\n",
    "            g += str(i_key)\n",
    "            #g += '\"' + str(i_key) + '\"'\n",
    "            if(i == 40):\n",
    "                break\n",
    "\n",
    "y = \"Y <- c(\" + tmpY + ',' + y + \")\"\n",
    "g = \"g <- as.factor(c(\" + tmpG + ',' + g + \"))\"\n",
    "print(y)\n",
    "print(\"\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.0 Flow_betweenness_centrality_HL\n",
      "84.0 AA_random, Network_centrality_HL\n",
      "85.0 Random_walk_betweenness_HL\n",
      "86.0 Stress_centrality_HL\n",
      "88.0 Core_decomposition_HL, Load_centrality_HL\n",
      "89.0 Alpha_LH, Centroid_centrality_HL, Lapacian_centrality_HL\n",
      "90.0 Average_distance_LH, Dangalchev_closeness_centrality_HL, Decay_centrality_HL, Degree_centrality_HL, Markov_centrality_HL, Political_independence_index_LH, Random_walk_closeness_HL, Semi_local_centrality_HL, Shortest_path_degree_HL, Strength_weighted_vertex_degree_HL\n",
      "91.0 Local_clustering_coefficients_HL, MNC_centrality_HL, Political_independence_index_HL\n",
      "92.0 BottleNeck_centrality_HL, Bridging_centrality_LH, Cross_clique_connectivity_HL, Lobby_index_HL, Topological_coefficient_LH\n",
      "93.0 Betweenness_HL, Diffusion_degree_HL, Leverage_centrality_HL, Path_centrality_HL, Shortest_path_betweenness_HL\n",
      "94.0 Communicability_betweenness_centrality_HL, Entropy_centrality_LH, MCC_centrality_HL\n",
      "95.0 Barycenter_centrality_HL, Closeness_Freeman_HL, Effectiveness_centrality_HL, Flow_betweenness_centrality_LH, LAC_HL, Lin_centrality_HL, Network_fragmentation_GeodesicDistanceWeighted_HL, Radiality_centrality_HL, SALSA_HL, Shortest_path_closeness_HL\n",
      "96.0 Closeness_VariantLatora_HL, Community_centrality_HL, Eccentricity_LH, Leverage_centrality_LH\n",
      "97.0 Alpha_HL, ClusterRank_HL, Neighborhood_connectivity_HL, Network_fragmentation_GeodesicDistanceWeighted_LH, Path_centrality_LH, Stress_centrality_LH, Subgraph_HL, Subgraph_LH\n",
      "98.0 Centroid_centrality_LH, EPC_HL, LAC_LH, Local_assortativity_HL\n",
      "99.0 ClusterRank_LH, DMNC_centrality_HL, Eigenvector_HL, Entropy_centrality_HL, EPC_LH, Kleinbergs_centrality_HITS_HL, Markov_centrality_LH, Random_walk_closeness_LH\n",
      "100.0 Communicability_betweenness_centrality_LH, Core_decomposition_LH, Eccentricity_HL, Effectiveness_centrality_LH, Local_assortativity_LH, Local_clustering_coefficients_LH, MNC_centrality_LH\n",
      "101.0 DMNC_centrality_LH, SALSA_LH, Semi_local_centrality_LH\n",
      "102.0 Bridging_centrality_HL, Random_walk_betweenness_LH\n",
      "103.0 Community_centrality_LH, Current_flow_closeness_centrality_HL, Information_centrality_HL, Topological_coefficient_HL\n",
      "104.0 Barycenter_centrality_LH, Closeness_Freeman_LH, Lin_centrality_LH, Radiality_centrality_LH, Shortest_path_closeness_LH\n",
      "105.0 Closeness_VariantLatora_LH, Current_flow_closeness_centrality_LH, Information_centrality_LH, Network_fragmentation_HL\n",
      "106.0 Average_distance_HL, Betweenness_LH, BottleNeck_centrality_LH, Dangalchev_closeness_centrality_LH, Decay_centrality_LH, Lobby_index_LH, Network_centrality_LH, Shortest_path_betweenness_LH\n",
      "107.0 Degree_centrality_LH, Eigenvector_LH, Kleinbergs_centrality_HITS_LH, Load_centrality_LH, MCC_centrality_LH, Neighborhood_connectivity_LH, Network_fragmentation_LH, Shortest_path_degree_LH, Strength_weighted_vertex_degree_LH\n",
      "108.0 Cross_clique_centrality_LH\n",
      "109.0 Diffusion_degree_LH\n",
      "111.0 Lapacian_centrality_LH\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(modeScore.keys()):\n",
    "    s = ''\n",
    "    for c in modeScore[key]:\n",
    "        if len(s):\n",
    "            s += ', '\n",
    "        s += c\n",
    "    print(key, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target best mode centrality: 51\n",
      "k <- c(\"AA_random\",\"Alpha_HL\",\"Alpha_LH\",\"Average_distance_HL\",\"Average_distance_LH\",\"Barycenter_centrality_HL\",\"Barycenter_centrality_LH\",\"Betweenness_HL\",\"Betweenness_LH\",\"BottleNeck_centrality_HL\",\"BottleNeck_centrality_LH\",\"Bridging_centrality_HL\",\"Bridging_centrality_LH\",\"Centroid_centrality_HL\",\"Centroid_centrality_LH\",\"Closeness_Freeman_HL\",\"Closeness_Freeman_LH\",\"Closeness_VariantLatora_HL\",\"Closeness_VariantLatora_LH\",\"ClusterRank_HL\",\"ClusterRank_LH\",\"Communicability_betweenness_centrality_HL\",\"Communicability_betweenness_centrality_LH\",\"Community_centrality_HL\",\"Community_centrality_LH\",\"Core_decomposition_HL\",\"Core_decomposition_LH\",\"Cross_clique_centrality_LH\",\"Cross_clique_connectivity_HL\",\"Current_flow_closeness_centrality_HL\",\"Current_flow_closeness_centrality_LH\",\"Dangalchev_closeness_centrality_HL\",\"Dangalchev_closeness_centrality_LH\",\"Decay_centrality_HL\",\"Decay_centrality_LH\",\"Degree_centrality_HL\",\"Degree_centrality_LH\",\"Diffusion_degree_HL\",\"Diffusion_degree_LH\",\"DMNC_centrality_HL\",\"DMNC_centrality_LH\",\"Eccentricity_HL\",\"Eccentricity_LH\",\"Effectiveness_centrality_HL\",\"Effectiveness_centrality_LH\",\"Eigenvector_HL\",\"Eigenvector_LH\",\"Entropy_centrality_HL\",\"Entropy_centrality_LH\",\"EPC_HL\",\"EPC_LH\",\"Flow_betweenness_centrality_HL\",\"Flow_betweenness_centrality_LH\",\"Information_centrality_HL\",\"Information_centrality_LH\",\"Kleinbergs_centrality_HITS_HL\",\"Kleinbergs_centrality_HITS_LH\",\"LAC_HL\",\"LAC_LH\",\"Lapacian_centrality_HL\",\"Lapacian_centrality_LH\",\"Leverage_centrality_HL\",\"Leverage_centrality_LH\",\"Lin_centrality_HL\",\"Lin_centrality_LH\",\"Load_centrality_HL\",\"Load_centrality_LH\",\"Lobby_index_HL\",\"Lobby_index_LH\",\"Local_assortativity_HL\",\"Local_assortativity_LH\",\"Local_clustering_coefficients_HL\",\"Local_clustering_coefficients_LH\",\"Markov_centrality_HL\",\"Markov_centrality_LH\",\"MCC_centrality_HL\",\"MCC_centrality_LH\",\"MNC_centrality_HL\",\"MNC_centrality_LH\",\"Neighborhood_connectivity_HL\",\"Neighborhood_connectivity_LH\",\"Network_centrality_HL\",\"Network_centrality_LH\",\"Network_fragmentation_GeodesicDistanceWeighted_HL\",\"Network_fragmentation_GeodesicDistanceWeighted_LH\",\"Network_fragmentation_HL\",\"Network_fragmentation_LH\",\"Path_centrality_HL\",\"Path_centrality_LH\",\"Political_independence_index_HL\",\"Political_independence_index_LH\",\"Radiality_centrality_HL\",\"Radiality_centrality_LH\",\"Random_walk_betweenness_HL\",\"Random_walk_betweenness_LH\",\"Random_walk_closeness_HL\",\"Random_walk_closeness_LH\",\"SALSA_HL\",\"SALSA_LH\",\"Semi_local_centrality_HL\",\"Semi_local_centrality_LH\",\"Shortest_path_betweenness_HL\",\"Shortest_path_betweenness_LH\",\"Shortest_path_closeness_HL\",\"Shortest_path_closeness_LH\",\"Shortest_path_degree_HL\",\"Shortest_path_degree_LH\",\"Strength_weighted_vertex_degree_HL\",\"Strength_weighted_vertex_degree_LH\",\"Stress_centrality_HL\",\"Stress_centrality_LH\",\"Subgraph_HL\",\"Subgraph_LH\",\"Topological_coefficient_HL\",\"Topological_coefficient_LH\")\n"
     ]
    }
   ],
   "source": [
    "k = ''\n",
    "for key in sorted(gkey.keys()):\n",
    "    if(len(k)):\n",
    "        k += ','\n",
    "    k += '\"' + gkey[key] + '\"'\n",
    "    if gkey[key] == 'Flow_betweenness_centrality_HL':\n",
    "        print('target best mode centrality:', key)\n",
    "\n",
    "k = 'k <- c(' + k + ')'\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_best <- c(80.0,93.0,89.0,74.0,98.0,104.0,84.0,90.0,99.0,83.0,87.0,91.0,83.0,95.0,71.0,84.0,96.0,87.0,90.0,92.0,98.0,101.0,85.0,90.0,88.0,91.0,96.0,77.0,95.0,97.0,87.0,92.0,83.0,94.0,99.0,89.0,102.0,84.0,80.0,95.0,90.0,104.0,87.0,87.0,100.0,101.0,84.0,95.0,84.0,85.0,92.0,94.0,87.0,104.0,87.0,91.0,88.0,82.0,92.0,91.0,98.0,99.0,84.0,99.0,100.0,94.0,93.0,75.0,106.0,95.0,96.0,112.0,106.0,96.0,106.0,83.0,100.0,84.0,85.0,85.0,98.0,95.0,99.0,103.0,125.0,119.0,95.0,102.0,96.0,77.0,99.0,99.0,97.0,99.0,86.0,93.0,105.0,97.0,110.0,82.0,98.0,104.0,98.0,101.0,101.0,107.0,92.0,107.0,105.0,97.0,86.0,96.0,95.0,100.0,97.0,91.0,100.0,102.0,92.0,106.0,91.0,79.0,95.0,78.0,93.0,96.0,84.0,91.0,99.0,77.0,96.0,88.0,82.0,96.0,88.0,89.0,92.0,98.0,104.0,84.0,81.0,102.0,79.0,102.0,88.0,98.0,89.0,96.0,102.0,98.0,87.0,94.0,94.0,89.0,100.0,83.0,91.0,89.0,82.0,86.0,106.0,92.0,109.0,111.0,116.0,111.0,102.0,106.0,103.0,84.0,84.0,102.0,94.0,109.0,98.0,98.0,116.0,103.0,106.0,97.0,108.0,99.0,82.0,116.0,111.0,106.0,114.0,97.0,106.0,104.0,110.0,109.0,103.0,101.0,110.0,97.0,110.0,104.0,101.0,109.0,94.0,90.0,105.0,90.0,109.0,100.0,95.0,90.0,107.0,74.0,90.0,95.0,94.0,97.0,76.0,90.0,105.0,90.0,88.0,99.0,97.0,89.0,86.0,100.0,105.0,99.0,98.0,96.0,107.0,97.0,107.0,99.0,92.0,98.0,99.0,92.0,106.0,98.0,85.0,95.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,89.0,87.0,82.0,79.0,100.0,98.0,87.0,89.0,97.0,70.0,82.0,85.0,80.0,93.0,79.0,90.0,94.0,88.0,90.0,90.0,87.0,93.0,84.0,88.0,93.0,95.0,86.0,77.0,98.0,95.0,93.0,101.0,89.0,85.0,92.0,90.0,93.0,80.0,83.0,83.0,94.0,100.0,110.0,104.0,119.0,107.0,95.0,103.0,103.0,94.0,90.0,99.0,108.0,102.0,93.0,101.0,102.0,98.0,106.0,99.0,90.0,98.0,88.0,104.0,96.0,115.0,112.0,101.0,113.0,88.0,117.0,105.0,107.0,114.0,111.0,106.0,115.0,106.0,95.0,111.0,82.0,78.0,91.0,79.0,101.0,98.0,78.0,85.0,92.0,68.0,84.0,92.0,79.0,94.0,76.0,79.0,97.0,72.0,92.0,89.0,93.0,86.0,90.0,97.0,98.0,90.0,85.0,75.0,100.0,96.0,95.0,93.0,77.0,88.0,96.0,87.0,101.0,91.0,87.0,92.0,106.0,92.0,109.0,95.0,119.0,105.0,99.0,108.0,117.0,83.0,107.0,110.0,90.0,100.0,79.0,90.0,99.0,97.0,101.0,106.0,106.0,104.0,98.0,110.0,95.0,101.0,118.0,105.0,113.0,106.0,108.0,109.0,96.0,104.0,112.0,99.0,114.0,113.0,94.0,109.0,108.0,86.0,98.0,95.0,115.0,100.0,86.0,109.0,105.0,81.0,87.0,98.0,97.0,99.0,93.0,102.0,105.0,95.0,109.0,94.0,99.0,102.0,89.0,101.0,105.0,102.0,101.0,87.0,116.0,105.0,91.0,101.0,86.0,94.0,102.0,99.0,110.0,99.0,88.0,102.0,89.0,87.0,97.0,95.0,100.0,108.0,92.0,89.0,111.0,78.0,92.0,92.0,89.0,98.0,75.0,94.0,101.0,92.0,104.0,93.0,89.0,99.0,86.0,95.0,106.0,103.0,91.0,86.0,108.0,95.0,103.0,110.0,92.0,101.0,85.0,99.0,107.0,99.0,86.0,93.0,92.0,83.0,98.0,92.0,109.0,101.0,92.0,93.0,96.0,79.0,89.0,91.0,87.0,88.0,72.0,91.0,98.0,93.0,94.0,100.0,100.0,89.0,89.0,92.0,105.0,95.0,98.0,93.0,104.0,88.0,96.0,101.0,82.0,95.0,91.0,89.0,97.0,90.0,88.0,94.0,95.0,97.0,105.0,85.0,128.0,113.0,97.0,107.0,100.0,103.0,100.0,102.0,105.0,106.0,99.0,101.0,105.0,98.0,106.0,100.0,101.0,105.0,97.0,104.0,107.0,109.0,96.0,102.0,110.0,96.0,98.0,118.0,114.0,104.0,113.0,98.0,111.0,110.0,98.0,103.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,94.0,84.0,102.0,86.0,107.0,101.0,93.0,87.0,99.0,77.0,90.0,96.0,91.0,100.0,76.0,90.0,109.0,91.0,96.0,98.0,96.0,97.0,88.0,103.0,107.0,93.0,103.0,92.0,104.0,97.0,98.0,105.0,84.0,95.0,96.0,87.0,105.0,95.0,91.0,100.0,105.0,102.0,116.0,106.0,127.0,112.0,99.0,105.0,107.0,85.0,101.0,100.0,96.0,102.0,78.0,88.0,102.0,100.0,111.0,103.0,108.0,108.0,90.0,117.0,112.0,106.0,120.0,89.0,111.0,105.0,113.0,103.0,104.0,105.0,113.0,97.0,107.0,106.0,99.0,104.0,97.0,90.0,97.0,94.0,116.0,107.0,91.0,97.0,101.0,80.0,87.0,92.0,95.0,98.0,93.0,97.0,99.0,93.0,101.0,97.0,96.0,102.0,81.0,99.0,100.0,94.0,104.0,108.0,105.0,96.0,100.0,102.0,85.0,93.0,96.0,87.0,98.0,93.0,82.0,96.0,98.0,94.0,99.0,97.0,126.0,111.0,105.0,108.0,107.0,103.0,96.0,101.0,94.0,99.0,99.0,91.0,93.0,98.0,107.0,99.0,100.0,97.0,85.0,103.0,107.0,102.0,109.0,96.0,104.0,110.0,106.0,107.0,96.0,110.0,112.0,97.0,104.0,104.0,88.0,111.0,84.0,80.0,104.0,82.0,109.0,102.0,85.0,94.0,98.0,72.0,89.0,97.0,85.0,94.0,78.0,91.0,99.0,87.0,92.0,94.0,94.0,89.0,86.0,102.0,94.0,87.0,97.0,93.0,92.0,92.0,98.0,102.0,83.0,93.0,93.0,81.0,103.0,90.0,91.0,90.0,100.0,105.0,109.0,100.0,107.0,100.0,96.0,113.0,103.0,108.0,100.0,110.0,93.0,109.0,92.0,89.0,112.0,96.0,115.0,102.0,106.0,111.0,88.0,108.0,114.0,104.0,115.0,113.0,107.0,103.0,105.0,111.0,93.0,103.0,109.0,93.0,106.0,99.0,92.0,106.0,81.0,79.0,95.0,79.0,104.0,96.0,92.0,91.0,97.0,89.0,85.0,97.0,84.0,97.0,76.0,80.0,104.0,86.0,96.0,90.0,96.0,90.0,73.0,94.0,109.0,98.0,93.0,83.0,101.0,83.0,89.0,99.0,93.0,93.0,94.0,84.0,98.0,96.0,80.0,90.0,90.0,100.0,109.0,107.0,125.0,108.0,103.0,104.0,105.0,98.0,103.0,101.0,92.0,103.0,97.0,107.0,105.0,96.0,105.0,103.0,111.0,102.0,95.0,101.0,107.0,109.0,113.0,97.0,110.0,106.0,106.0,112.0,95.0,106.0,104.0,94.0,112.0,116.0,90.0,104.0,101.0,87.0,102.0,93.0,109.0,95.0,89.0,100.0,91.0,93.0,96.0,99.0,88.0,95.0,93.0,95.0,95.0,102.0,100.0,102.0,93.0,98.0,88.0,103.0,106.0,96.0,105.0,89.0,103.0,105.0,111.0,100.0,88.0,97.0,102.0,92.0,105.0,96.0,83.0,88.0,106.0,109.0,93.0,98.0,117.0,116.0,104.0,107.0,113.0,93.0,102.0,99.0,104.0,98.0,97.0,107.0,106.0,102.0,110.0,100.0,104.0,94.0,91.0,121.0,118.0,106.0,117.0,101.0,115.0,96.0,119.0,115.0,100.0,109.0,111.0,101.0,101.0,100.0,99.0,110.0,101.0,102.0,107.0,105.0,130.0,107.0,103.0,108.0,106.0,105.0,103.0,99.0,96.0,113.0,104.0,106.0,114.0,103.0,102.0,108.0,110.0,99.0,99.0,111.0,113.0,116.0,110.0,108.0,112.0,108.0,93.0,115.0,93.0,106.0,114.0,101.0,114.0,108.0,101.0,109.0,86.0,83.0,101.0,91.0,106.0,100.0,94.0,95.0,90.0,86.0,92.0,95.0,90.0,103.0,67.0,92.0,104.0,86.0,94.0,98.0,96.0,87.0,80.0,96.0,99.0,93.0,98.0,92.0,92.0,95.0,101.0,96.0,98.0,89.0,98.0,90.0,98.0,100.0,92.0,86.0,84.0,88.0,96.0,83.0,106.0,105.0,90.0,90.0,103.0,80.0,87.0,91.0,86.0,97.0,74.0,92.0,103.0,93.0,91.0,99.0,98.0,90.0,85.0,101.0,103.0,94.0,102.0,90.0,99.0,87.0,101.0,103.0,81.0,103.0,96.0,92.0,97.0,98.0,95.0,94.0,99.0,102.0,107.0,98.0,117.0,111.0,100.0,110.0,110.0,103.0,108.0,107.0,93.0,105.0,98.0,103.0,105.0,93.0,104.0,93.0,102.0,106.0,91.0,117.0,117.0,105.0,119.0,104.0,104.0,97.0,102.0,117.0,105.0,102.0,94.0,111.0,109.0,105.0,104.0,107.0,97.0,84.0,104.0,88.0,112.0,100.0,93.0,90.0,98.0,84.0,88.0,92.0,88.0,90.0,72.0,97.0,107.0,90.0,97.0,98.0,97.0,90.0,85.0,103.0,109.0,103.0,101.0,98.0,102.0,98.0,98.0,103.0,90.0,95.0,91.0,94.0,105.0,101.0,93.0,99.0,97.0,103.0,93.0,106.0,121.0,111.0,102.0,102.0,104.0,79.0,97.0,104.0,99.0,106.0,86.0,103.0,119.0,108.0,96.0,105.0,106.0,106.0,95.0,112.0,106.0,110.0,103.0,110.0,108.0,109.0,108.0,108.0,99.0,111.0,100.0,106.0,113.0,110.0,91.0,106.0,97.0,84.0,104.0,88.0,112.0,100.0,93.0,90.0,98.0,84.0,88.0,92.0,88.0,90.0,72.0,97.0,107.0,90.0,97.0,98.0,97.0,90.0,85.0,103.0,109.0,103.0,101.0,98.0,102.0,98.0,98.0,103.0,90.0,95.0,91.0,94.0,105.0,101.0,93.0,99.0,97.0,103.0,93.0,106.0,121.0,111.0,102.0,102.0,104.0,79.0,97.0,104.0,99.0,106.0,86.0,103.0,119.0,108.0,96.0,105.0,106.0,106.0,95.0,112.0,106.0,110.0,103.0,110.0,108.0,109.0,108.0,108.0,99.0,111.0,100.0,106.0,113.0,110.0,91.0,106.0,89.0,82.0,96.0,84.0,107.0,97.0,88.0,90.0,94.0,77.0,92.0,90.0,86.0,91.0,68.0,90.0,109.0,90.0,92.0,93.0,96.0,88.0,79.0,100.0,102.0,96.0,94.0,94.0,98.0,87.0,94.0,102.0,84.0,90.0,97.0,84.0,99.0,92.0,88.0,85.0,94.0,107.0,108.0,106.0,116.0,117.0,99.0,105.0,102.0,93.0,110.0,107.0,100.0,111.0,80.0,101.0,115.0,101.0,109.0,103.0,108.0,98.0,89.0,120.0,125.0,111.0,122.0,107.0,109.0,105.0,111.0,114.0,91.0,108.0,98.0,108.0,107.0,105.0,90.0,114.0,90.0,91.0,109.0,92.0,107.0,98.0,97.0,93.0,104.0,84.0,87.0,97.0,91.0,95.0,72.0,91.0,104.0,88.0,96.0,95.0,102.0,89.0,82.0,103.0,106.0,92.0,100.0,93.0,106.0,93.0,94.0,99.0,93.0,101.0,94.0,86.0,106.0,96.0,89.0,96.0,96.0,108.0,101.0,115.0,106.0,102.0,109.0,95.0,109.0,96.0,103.0,105.0,102.0,106.0,84.0,103.0,111.0,109.0,107.0,104.0,108.0,106.0,90.0,110.0,103.0,116.0,113.0,96.0,109.0,103.0,109.0,115.0,93.0,112.0,102.0,97.0,105.0,104.0,95.0,114.0,87.0,95.0,100.0,89.0,119.0,115.0,95.0,92.0,99.0,77.0,98.0,97.0,98.0,111.0,92.0,93.0,102.0,102.0,98.0,99.0,99.0,95.0,92.0,105.0,99.0,104.0,105.0,100.0,100.0,101.0,102.0,102.0,92.0,103.0,97.0,88.0,110.0,99.0,93.0,102.0,98.0,81.0,97.0,101.0,118.0,107.0,104.0,102.0,95.0,80.0,103.0,112.0,98.0,97.0,88.0,101.0,107.0,95.0,101.0,99.0,98.0,100.0,89.0,102.0,108.0,107.0,113.0,101.0,108.0,104.0,102.0,109.0,88.0,95.0,112.0,94.0,109.0,112.0,99.0,100.0,110.0,90.0,95.0,105.0,120.0,112.0,103.0,107.0,100.0,101.0,103.0,109.0,96.0,100.0,96.0,89.0,95.0,93.0,98.0,89.0,100.0,105.0,90.0,102.0,110.0,109.0,109.0,111.0,98.0,96.0,97.0,100.0,94.0,107.0,114.0,91.0,101.0,108.0,90.0,99.0,105.0,84.0,102.0,88.0,111.0,109.0,85.0,97.0,97.0,79.0,92.0,97.0,96.0,102.0,95.0,93.0,96.0,89.0,102.0,95.0,96.0,92.0,82.0,96.0,98.0,97.0,99.0,89.0,101.0,107.0,100.0,109.0,94.0,97.0,91.0,91.0,109.0,96.0,96.0,89.0,81.0,75.0,95.0,87.0,100.0,96.0,82.0,89.0,92.0,77.0,81.0,95.0,80.0,94.0,67.0,80.0,99.0,87.0,86.0,90.0,89.0,87.0,80.0,95.0,97.0,95.0,85.0,79.0,96.0,86.0,98.0,105.0,91.0,92.0,95.0,90.0,97.0,91.0,91.0,85.0,96.0,97.0,106.0,97.0,117.0,116.0,103.0,107.0,104.0,100.0,108.0,102.0,95.0,121.0,96.0,105.0,113.0,101.0,108.0,101.0,100.0,100.0,95.0,97.0,111.0,110.0,107.0,98.0,114.0,105.0,92.0,116.0,100.0,106.0,99.0,102.0,100.0,92.0,93.0,107.0,88.0,95.0,103.0,96.0,112.0,106.0,94.0,98.0,103.0,86.0,99.0,102.0,97.0,91.0,87.0,91.0,102.0,99.0,107.0,94.0,99.0,92.0,84.0,112.0,113.0,97.0,99.0,96.0,105.0,94.0,100.0,102.0,92.0,101.0,99.0,91.0,107.0,104.0,89.0,93.0,107.0,101.0,110.0,101.0,113.0,110.0,96.0,110.0,111.0,84.0,95.0,102.0,111.0,102.0,83.0,97.0,104.0,109.0,114.0,101.0,107.0,108.0,98.0,106.0,110.0,118.0,110.0,112.0,115.0,107.0,107.0,106.0,91.0,109.0,107.0,104.0,113.0,110.0,102.0,107.0,92.0,91.0,99.0,96.0,123.0,99.0,98.0,99.0,105.0,92.0,100.0,108.0,96.0,99.0,99.0,96.0,107.0,101.0,105.0,95.0,100.0,100.0,92.0,104.0,107.0,108.0,102.0,98.0,111.0,112.0,113.0,101.0,86.0,104.0,107.0,99.0,106.0,89.0,95.0,105.0,94.0,87.0,87.0,95.0,114.0,105.0,90.0,103.0,101.0,80.0,94.0,98.0,90.0,97.0,81.0,95.0,94.0,84.0,93.0,90.0,91.0,94.0,89.0,102.0,99.0,95.0,97.0,81.0,98.0,103.0,86.0,105.0,97.0,99.0,106.0,94.0,101.0,97.0,90.0,108.0,90.0,87.0,86.0,90.0,110.0,99.0,91.0,90.0,91.0,79.0,90.0,97.0,82.0,94.0,72.0,88.0,106.0,88.0,99.0,98.0,97.0,91.0,82.0,108.0,98.0,100.0,98.0,96.0,106.0,91.0,103.0,98.0,80.0,87.0,98.0,94.0,98.0,100.0,86.0,102.0,98.0,99.0,108.0,89.0,119.0,107.0,104.0,106.0,110.0,97.0,104.0,104.0,101.0,103.0,97.0,111.0,107.0,105.0,109.0,105.0,110.0,110.0,89.0,116.0,112.0,105.0,109.0,107.0,105.0,99.0,118.0,115.0,96.0,101.0,110.0,99.0,109.0,99.0,102.0,104.0,101.0,100.0,100.0,94.0,118.0,111.0,100.0,102.0,99.0,95.0,101.0,107.0,103.0,109.0,96.0,99.0,102.0,104.0,102.0,95.0,99.0,95.0,84.0,107.0,102.0,97.0,101.0,95.0,123.0,89.0,95.0,115.0,94.0,111.0,93.0,102.0,106.0,92.0,92.0,104.0,84.0,88.0,96.0,83.0,106.0,105.0,90.0,90.0,103.0,80.0,87.0,91.0,86.0,97.0,74.0,92.0,103.0,93.0,91.0,99.0,98.0,90.0,85.0,101.0,103.0,94.0,102.0,90.0,99.0,87.0,101.0,103.0,81.0,103.0,96.0,92.0,97.0,98.0,95.0,94.0,99.0,102.0,107.0,98.0,117.0,111.0,100.0,110.0,110.0,103.0,108.0,107.0,93.0,105.0,98.0,103.0,105.0,93.0,104.0,93.0,102.0,106.0,91.0,117.0,117.0,105.0,119.0,104.0,104.0,97.0,102.0,117.0,105.0,102.0,94.0,111.0,109.0,105.0,104.0,107.0,88.0,95.0,103.0,96.0,112.0,106.0,94.0,98.0,103.0,86.0,99.0,102.0,97.0,91.0,87.0,91.0,102.0,99.0,107.0,94.0,99.0,92.0,84.0,112.0,113.0,97.0,99.0,96.0,105.0,94.0,100.0,102.0,92.0,101.0,99.0,91.0,107.0,104.0,89.0,93.0,107.0,101.0,110.0,101.0,113.0,110.0,96.0,110.0,111.0,84.0,95.0,102.0,111.0,102.0,83.0,97.0,104.0,109.0,114.0,101.0,107.0,108.0,98.0,106.0,110.0,118.0,110.0,112.0,115.0,107.0,107.0,106.0,91.0,109.0,107.0,104.0,113.0,110.0,102.0,107.0,82.0,85.0,102.0,94.0,111.0,100.0,92.0,95.0,102.0,84.0,90.0,91.0,88.0,93.0,87.0,88.0,99.0,93.0,96.0,91.0,101.0,92.0,81.0,99.0,106.0,97.0,103.0,90.0,97.0,95.0,95.0,103.0,81.0,99.0,95.0,91.0,105.0,99.0,88.0,94.0,90.0,102.0,106.0,102.0,118.0,110.0,98.0,101.0,96.0,103.0,106.0,111.0,92.0,109.0,103.0,98.0,98.0,102.0,111.0,92.0,99.0,104.0,97.0,112.0,107.0,98.0,119.0,99.0,116.0,102.0,117.0,110.0,98.0,102.0,114.0,101.0,113.0,96.0,98.0,107.0,89.0,86.0,102.0,89.0,111.0,107.0,94.0,92.0,98.0,83.0,93.0,96.0,89.0,91.0,77.0,86.0,110.0,86.0,95.0,94.0,100.0,88.0,84.0,97.0,104.0,90.0,98.0,97.0,101.0,94.0,99.0,97.0,94.0,95.0,87.0,89.0,103.0,92.0,89.0,88.0,97.0,100.0,109.0,91.0,117.0,109.0,111.0,108.0,111.0,87.0,97.0,103.0,90.0,107.0,95.0,106.0,113.0,100.0,114.0,104.0,111.0,99.0,92.0,103.0,117.0,97.0,110.0,104.0,114.0,108.0,107.0,111.0,93.0,103.0,108.0,104.0,102.0,102.0,98.0,112.0,83.0,87.0,95.0,76.0,106.0,96.0,78.0,88.0,92.0,65.0,85.0,91.0,84.0,95.0,64.0,85.0,93.0,87.0,91.0,90.0,86.0,97.0,78.0,93.0,94.0,88.0,91.0,79.0,92.0,82.0,98.0,97.0,86.0,96.0,99.0,93.0,93.0,96.0,88.0,94.0,102.0,91.0,106.0,91.0,120.0,111.0,108.0,111.0,106.0,94.0,98.0,99.0,96.0,105.0,101.0,97.0,101.0,101.0,112.0,97.0,100.0,110.0,91.0,114.0,96.0,104.0,107.0,96.0,104.0,100.0,110.0,106.0,87.0,105.0,106.0,96.0,107.0,104.0,93.0,104.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,92.0,81.0,82.0,74.0,96.0,94.0,88.0,86.0,99.0,71.0,86.0,89.0,84.0,101.0,77.0,94.0,95.0,91.0,89.0,97.0,90.0,97.0,84.0,98.0,95.0,88.0,85.0,79.0,98.0,92.0,98.0,100.0,88.0,85.0,89.0,88.0,97.0,89.0,82.0,91.0,95.0,103.0,109.0,98.0,108.0,109.0,93.0,104.0,107.0,100.0,91.0,112.0,105.0,107.0,96.0,104.0,99.0,105.0,103.0,96.0,101.0,107.0,91.0,107.0,108.0,106.0,114.0,89.0,118.0,102.0,103.0,111.0,107.0,97.0,101.0,98.0,108.0,111.0,98.0,107.0,89.0,81.0,95.0,92.0,99.0,97.0,93.0,95.0,92.0,90.0,92.0,91.0,82.0,96.0,74.0,85.0,93.0,94.0,95.0,95.0,95.0,92.0,81.0,101.0,101.0,93.0,98.0,91.0,114.0,95.0,104.0,97.0,81.0,97.0,92.0,92.0,101.0,88.0,87.0,92.0,106.0,110.0,102.0,106.0,126.0,107.0,99.0,109.0,106.0,96.0,101.0,97.0,98.0,104.0,92.0,96.0,107.0,109.0,109.0,107.0,107.0,111.0,93.0,111.0,107.0,105.0,101.0,109.0,115.0,110.0,106.0,106.0,105.0,110.0,105.0,97.0,109.0,104.0,98.0,112.0,97.0,94.0,93.0,93.0,115.0,105.0,98.0,91.0,99.0,100.0,98.0,88.0,94.0,98.0,86.0,90.0,103.0,89.0,104.0,88.0,88.0,100.0,83.0,119.0,90.0,102.0,100.0,86.0,101.0,96.0,90.0,105.0,89.0,98.0,88.0,92.0,103.0,111.0,94.0,98.0,95.0,87.0,100.0,96.0,108.0,108.0,101.0,98.0,99.0,82.0,109.0,100.0,97.0,99.0,80.0,87.0,91.0,90.0,110.0,91.0,86.0,100.0,91.0,98.0,109.0,104.0,95.0,86.0,104.0,96.0,85.0,105.0,100.0,100.0,97.0,89.0,105.0,100.0,83.0,99.0,94.0,97.0,95.0,93.0,116.0,103.0,97.0,95.0,106.0,98.0,101.0,92.0,91.0,106.0,76.0,96.0,105.0,93.0,94.0,94.0,96.0,104.0,84.0,112.0,105.0,108.0,97.0,98.0,106.0,91.0,103.0,106.0,95.0,102.0,92.0,91.0,107.0,98.0,91.0,96.0,93.0,92.0,96.0,100.0,112.0,108.0,99.0,103.0,99.0,87.0,103.0,103.0,95.0,100.0,84.0,97.0,105.0,95.0,95.0,101.0,92.0,96.0,85.0,114.0,108.0,101.0,101.0,103.0,106.0,100.0,101.0,102.0,99.0,96.0,100.0,91.0,118.0,102.0,87.0,98.0,84.0,88.0,97.0,82.0,104.0,96.0,90.0,95.0,102.0,81.0,91.0,93.0,89.0,96.0,74.0,100.0,102.0,100.0,90.0,99.0,98.0,94.0,87.0,101.0,108.0,94.0,99.0,91.0,97.0,87.0,94.0,103.0,90.0,103.0,104.0,92.0,98.0,98.0,90.0,94.0,96.0,94.0,108.0,99.0,117.0,107.0,100.0,103.0,97.0,102.0,95.0,105.0,93.0,105.0,85.0,104.0,109.0,101.0,107.0,109.0,102.0,106.0,99.0,117.0,112.0,105.0,118.0,99.0,104.0,97.0,98.0,106.0,101.0,100.0,92.0,91.0,111.0,100.0,95.0,91.0,87.0,91.0,99.0,93.0,103.0,102.0,96.0,94.0,94.0,82.0,89.0,99.0,88.0,93.0,70.0,89.0,100.0,93.0,97.0,92.0,94.0,86.0,82.0,97.0,100.0,90.0,96.0,100.0,94.0,92.0,99.0,93.0,84.0,97.0,91.0,90.0,101.0,94.0,83.0,86.0,104.0,85.0,108.0,98.0,118.0,107.0,101.0,107.0,107.0,101.0,102.0,112.0,100.0,103.0,86.0,91.0,112.0,98.0,101.0,108.0,107.0,110.0,92.0,104.0,103.0,110.0,111.0,100.0,112.0,104.0,109.0,106.0,104.0,107.0,109.0,109.0,122.0,110.0,97.0,119.0,90.0,87.0,104.0,90.0,114.0,96.0,89.0,100.0,97.0,86.0,91.0,91.0,88.0,95.0,82.0,95.0,102.0,93.0,98.0,91.0,98.0,92.0,84.0,102.0,101.0,95.0,98.0,91.0,104.0,89.0,101.0,99.0,76.0,96.0,89.0,90.0,102.0,94.0,81.0,84.0,97.0,100.0,103.0,91.0,117.0,115.0,102.0,110.0,98.0,89.0,94.0,105.0,100.0,105.0,96.0,112.0,112.0,91.0,114.0,102.0,100.0,99.0,89.0,109.0,99.0,109.0,110.0,104.0,118.0,110.0,107.0,111.0,94.0,112.0,104.0,101.0,114.0,97.0,93.0,119.0,94.0,99.0,108.0,104.0,119.0,110.0,94.0,107.0,107.0,92.0,101.0,99.0,100.0,101.0,97.0,95.0,108.0,95.0,98.0,88.0,97.0,102.0,92.0,97.0,106.0,106.0,101.0,97.0,109.0,104.0,105.0,102.0,89.0,109.0,101.0,96.0,102.0,110.0,93.0,92.0,94.0,96.0,100.0,96.0,124.0,102.0,107.0,98.0,108.0,88.0,100.0,103.0,98.0,107.0,88.0,90.0,110.0,102.0,95.0,104.0,104.0,92.0,95.0,104.0,107.0,107.0,107.0,101.0,114.0,100.0,108.0,107.0,97.0,102.0,110.0,90.0,107.0,108.0,92.0,100.0,85.0,87.0,96.0,94.0,108.0,103.0,86.0,89.0,99.0,86.0,88.0,95.0,87.0,94.0,67.0,92.0,102.0,89.0,99.0,94.0,89.0,87.0,84.0,98.0,99.0,96.0,97.0,80.0,100.0,91.0,98.0,102.0,83.0,93.0,92.0,84.0,102.0,92.0,84.0,84.0,89.0,100.0,105.0,93.0,126.0,106.0,107.0,111.0,105.0,85.0,94.0,99.0,100.0,104.0,106.0,100.0,108.0,98.0,98.0,97.0,106.0,97.0,96.0,108.0,115.0,105.0,91.0,94.0,114.0,98.0,113.0,106.0,97.0,101.0,108.0,99.0,101.0,106.0,98.0,104.0,93.0,86.0,95.0,81.0,103.0,103.0,91.0,92.0,101.0,84.0,96.0,95.0,82.0,100.0,79.0,94.0,96.0,85.0,94.0,100.0,95.0,103.0,79.0,97.0,108.0,95.0,88.0,86.0,103.0,97.0,95.0,99.0,93.0,102.0,96.0,95.0,103.0,97.0,82.0,93.0,97.0,97.0,108.0,89.0,117.0,114.0,100.0,109.0,104.0,81.0,92.0,112.0,102.0,103.0,87.0,89.0,106.0,101.0,104.0,100.0,99.0,101.0,91.0,107.0,104.0,107.0,114.0,97.0,111.0,97.0,108.0,116.0,109.0,104.0,96.0,102.0,113.0,101.0,88.0,95.0,97.0,89.0,98.0,95.0,106.0,105.0,95.0,105.0,99.0,90.0,81.0,93.0,85.0,94.0,92.0,97.0,97.0,105.0,102.0,90.0,88.0,94.0,81.0,103.0,100.0,99.0,96.0,81.0,100.0,95.0,111.0,109.0,105.0,95.0,102.0,94.0,109.0,105.0,96.0,104.0,102.0,100.0,87.0,102.0,119.0,113.0,102.0,107.0,90.0,97.0,98.0,95.0,91.0,107.0,91.0,92.0,108.0,98.0,110.0,100.0,106.0,106.0,91.0,107.0,101.0,111.0,100.0,94.0,113.0,107.0,98.0,109.0,95.0,100.0,109.0,99.0,107.0,106.0,88.0,104.0,90.0,87.0,105.0,86.0,113.0,100.0,87.0,93.0,106.0,79.0,87.0,92.0,98.0,101.0,90.0,91.0,102.0,79.0,104.0,106.0,98.0,91.0,89.0,102.0,100.0,92.0,102.0,93.0,110.0,93.0,92.0,104.0,83.0,95.0,103.0,93.0,102.0,94.0,90.0,99.0,95.0,96.0,101.0,96.0,110.0,106.0,97.0,100.0,108.0,99.0,105.0,106.0,102.0,103.0,92.0,98.0,97.0,99.0,96.0,103.0,98.0,105.0,97.0,112.0,116.0,106.0,115.0,101.0,118.0,99.0,90.0,116.0,94.0,93.0,107.0,98.0,104.0,108.0,97.0,95.0,86.0,92.0,99.0,79.0,112.0,103.0,84.0,95.0,103.0,74.0,106.0,91.0,89.0,103.0,84.0,97.0,100.0,96.0,85.0,93.0,99.0,95.0,87.0,108.0,91.0,88.0,110.0,92.0,111.0,101.0,96.0,103.0,99.0,101.0,99.0,91.0,105.0,96.0,78.0,91.0,105.0,97.0,89.0,76.0,103.0,101.0,86.0,90.0,99.0,75.0,97.0,101.0,97.0,99.0,91.0,87.0,103.0,79.0,109.0,83.0,90.0,100.0,79.0,115.0,108.0,91.0,86.0,99.0,109.0,90.0,85.0,104.0,83.0,94.0,94.0,83.0,94.0,80.0,90.0,89.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,85.0,85.0,101.0,88.0,104.0,98.0,93.0,91.0,97.0,79.0,90.0,98.0,93.0,95.0,69.0,92.0,109.0,92.0,95.0,94.0,95.0,91.0,82.0,99.0,101.0,89.0,94.0,83.0,98.0,93.0,106.0,97.0,85.0,97.0,84.0,89.0,102.0,91.0,87.0,87.0,98.0,107.0,116.0,90.0,119.0,109.0,109.0,114.0,99.0,107.0,103.0,111.0,104.0,113.0,102.0,100.0,111.0,105.0,98.0,101.0,105.0,102.0,100.0,107.0,99.0,113.0,108.0,100.0,110.0,105.0,114.0,110.0,103.0,114.0,112.0,98.0,102.0,103.0,102.0,111.0,84.0,88.0,97.0,82.0,104.0,96.0,90.0,95.0,102.0,81.0,91.0,93.0,89.0,96.0,74.0,100.0,102.0,100.0,90.0,99.0,98.0,94.0,87.0,101.0,108.0,94.0,99.0,91.0,97.0,87.0,94.0,103.0,90.0,103.0,104.0,92.0,98.0,98.0,90.0,94.0,96.0,94.0,108.0,99.0,117.0,107.0,100.0,103.0,97.0,102.0,95.0,105.0,93.0,105.0,85.0,104.0,109.0,101.0,107.0,109.0,102.0,106.0,99.0,117.0,112.0,105.0,118.0,99.0,104.0,97.0,98.0,106.0,101.0,100.0,92.0,91.0,111.0,100.0,95.0,91.0,100.0,99.0,96.0,91.0,121.0,106.0,91.0,96.0,95.0,101.0,95.0,107.0,95.0,104.0,88.0,98.0,104.0,95.0,96.0,101.0,93.0,107.0,92.0,108.0,104.0,96.0,93.0,99.0,105.0,98.0,97.0,108.0,87.0,107.0,96.0,95.0,105.0,108.0,85.0,101.0,101.0,107.0,104.0,102.0,116.0,116.0,103.0,105.0,100.0,90.0,99.0,107.0,98.0,100.0,97.0,95.0,107.0,101.0,112.0,100.0,108.0,101.0,91.0,109.0,113.0,108.0,123.0,92.0,112.0,104.0,112.0,104.0,101.0,110.0,110.0,104.0,106.0,108.0,95.0,119.0,85.0,86.0,98.0,96.0,114.0,99.0,92.0,92.0,103.0,87.0,93.0,97.0,90.0,94.0,78.0,92.0,106.0,98.0,100.0,95.0,96.0,91.0,79.0,104.0,109.0,93.0,101.0,95.0,110.0,97.0,101.0,107.0,90.0,95.0,99.0,89.0,100.0,99.0,90.0,89.0,96.0,101.0,106.0,100.0,128.0,103.0,96.0,101.0,109.0,85.0,108.0,107.0,100.0,103.0,98.0,108.0,115.0,108.0,102.0,101.0,101.0,103.0,92.0,115.0,118.0,110.0,97.0,114.0,104.0,105.0,114.0,114.0,94.0,102.0,106.0,101.0,111.0,108.0,98.0,98.0,89.0,87.0,82.0,79.0,100.0,98.0,87.0,89.0,97.0,70.0,82.0,85.0,80.0,93.0,79.0,90.0,94.0,88.0,90.0,90.0,87.0,93.0,84.0,88.0,93.0,95.0,86.0,77.0,98.0,95.0,93.0,101.0,89.0,85.0,92.0,90.0,93.0,80.0,83.0,83.0,94.0,100.0,110.0,104.0,119.0,107.0,95.0,103.0,103.0,94.0,90.0,99.0,108.0,102.0,93.0,101.0,102.0,98.0,106.0,99.0,90.0,98.0,88.0,104.0,96.0,115.0,112.0,101.0,113.0,88.0,117.0,105.0,107.0,114.0,111.0,106.0,115.0,106.0,95.0,111.0,90.0,90.0,101.0,91.0,104.0,97.0,98.0,95.0,107.0,83.0,87.0,95.0,92.0,98.0,76.0,96.0,105.0,95.0,97.0,94.0,99.0,89.0,87.0,101.0,103.0,101.0,100.0,94.0,108.0,98.0,108.0,99.0,90.0,98.0,102.0,88.0,106.0,94.0,95.0,97.0,95.0,91.0,109.0,102.0,126.0,112.0,104.0,105.0,111.0,96.0,94.0,108.0,90.0,97.0,90.0,105.0,108.0,107.0,104.0,95.0,101.0,108.0,89.0,120.0,111.0,112.0,101.0,88.0,116.0,112.0,113.0,104.0,105.0,103.0,96.0,103.0,110.0,103.0,98.0,104.0,89.0,82.0,96.0,84.0,107.0,97.0,88.0,90.0,94.0,77.0,92.0,90.0,86.0,91.0,68.0,90.0,109.0,90.0,92.0,93.0,96.0,88.0,79.0,100.0,102.0,96.0,94.0,94.0,98.0,87.0,94.0,102.0,84.0,90.0,97.0,84.0,99.0,92.0,88.0,85.0,94.0,107.0,108.0,106.0,116.0,117.0,99.0,105.0,102.0,93.0,110.0,107.0,100.0,111.0,80.0,101.0,115.0,101.0,109.0,103.0,108.0,98.0,89.0,120.0,125.0,111.0,122.0,107.0,109.0,105.0,111.0,114.0,91.0,108.0,98.0,108.0,107.0,105.0,90.0,114.0,89.0,82.0,96.0,84.0,107.0,97.0,88.0,90.0,94.0,77.0,92.0,90.0,86.0,91.0,68.0,90.0,109.0,90.0,92.0,93.0,96.0,88.0,79.0,100.0,102.0,96.0,94.0,94.0,98.0,87.0,94.0,102.0,84.0,90.0,97.0,84.0,99.0,92.0,88.0,85.0,94.0,107.0,108.0,106.0,116.0,117.0,99.0,105.0,102.0,93.0,110.0,107.0,100.0,111.0,80.0,101.0,115.0,101.0,109.0,103.0,108.0,98.0,89.0,120.0,125.0,111.0,122.0,107.0,109.0,105.0,111.0,114.0,91.0,108.0,98.0,108.0,107.0,105.0,90.0,114.0,87.0,86.0,97.0,80.0,96.0,89.0,83.0,85.0,88.0,73.0,84.0,91.0,82.0,96.0,90.0,90.0,94.0,86.0,92.0,96.0,82.0,91.0,81.0,86.0,100.0,90.0,86.0,88.0,100.0,97.0,92.0,94.0,83.0,90.0,99.0,88.0,98.0,95.0,84.0,87.0,101.0,90.0,103.0,104.0,113.0,97.0,102.0,109.0,98.0,85.0,98.0,101.0,102.0,100.0,99.0,88.0,104.0,109.0,95.0,99.0,90.0,100.0,91.0,120.0,115.0,111.0,93.0,94.0,111.0,112.0,96.0,113.0,99.0,96.0,97.0,107.0,112.0,111.0,97.0,103.0,88.0,82.0,103.0,99.0,112.0,103.0,95.0,97.0,99.0,87.0,97.0,97.0,95.0,88.0,83.0,91.0,102.0,95.0,96.0,98.0,97.0,90.0,79.0,107.0,104.0,97.0,103.0,97.0,105.0,95.0,101.0,104.0,84.0,111.0,95.0,87.0,109.0,100.0,86.0,96.0,101.0,95.0,108.0,97.0,125.0,105.0,95.0,103.0,106.0,91.0,110.0,103.0,97.0,108.0,99.0,97.0,119.0,96.0,118.0,96.0,102.0,113.0,97.0,109.0,104.0,103.0,110.0,95.0,119.0,108.0,114.0,116.0,94.0,111.0,106.0,90.0,112.0,109.0,91.0,104.0,105.0,102.0,103.0,95.0,117.0,104.0,105.0,105.0,103.0,84.0,96.0,107.0,98.0,105.0,94.0,101.0,111.0,101.0,115.0,108.0,109.0,111.0,90.0,103.0,109.0,102.0,117.0,104.0,112.0,103.0,106.0,106.0,104.0,112.0,110.0,100.0,117.0,106.0,96.0,99.0,84.0,90.0,97.0,75.0,102.0,98.0,87.0,89.0,93.0,76.0,92.0,89.0,87.0,98.0,68.0,92.0,102.0,92.0,94.0,99.0,99.0,91.0,87.0,89.0,110.0,99.0,99.0,81.0,105.0,93.0,102.0,99.0,92.0,96.0,103.0,88.0,107.0,102.0,81.0,92.0)\n",
      "\n",
      "g_best <- as.factor(c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,43,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,45,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,57,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,58,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,69,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,70,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,73,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,76,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,77,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,79,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,81,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,82,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,84,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,85,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,87,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,88,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,89,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,91,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,93,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,94,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,96,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,99,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,105,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,108,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,110,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,111,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,112,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,113,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114))\n"
     ]
    }
   ],
   "source": [
    "# Extract variables for R analysis\n",
    "y = ''\n",
    "g = ''\n",
    "\n",
    "tmpY = ''\n",
    "tmpG = ''\n",
    "\n",
    "\n",
    "gkey = {}\n",
    "\n",
    "modeScore = {} # list of centralities for each modal edges cut value\n",
    "\n",
    "# this is to set the CONTROL for the R statistical tests (control = first centrality data that has to go in)\n",
    "\n",
    "# not sure - this would probably be the Leverage_Centrality_HL which has technically\n",
    "# the best modal score\n",
    "whichIDToKeepAsZero = 51\n",
    "\n",
    "for i_key, key in enumerate(list(centralities.keys())):\n",
    "    if(i_key < whichIDToKeepAsZero):\n",
    "        gkey[i_key + 1] = key\n",
    "    elif(i_key == whichIDToKeepAsZero):\n",
    "        gkey[0] = key\n",
    "    else:\n",
    "        gkey[i_key] = key\n",
    "        \n",
    "    centrality = centralities[key]\n",
    "    \n",
    "    # extract Edges Cut mode value\n",
    "    modescore = centrality.modeScores[2].split(':')\n",
    "    mecut = float(modescore[0]) # value of mode of edges cut\n",
    "    mcount = int(modescore[1]) # number of experiments with this modal value of edges cut\n",
    "    \n",
    "\n",
    "    # store each centrality based on their modal value of edges cut\n",
    "    #overallmodescore = mecut / mcount\n",
    "    overallmodescore = mecut\n",
    "    if overallmodescore in modeScore:\n",
    "        modeScore[overallmodescore].append(key)\n",
    "    else:\n",
    "        modeScore[overallmodescore] = [key]\n",
    "    \n",
    "    for i, score in enumerate(centrality.scores):\n",
    "        edges_cut = score[2]\n",
    "        \n",
    "        if(i_key == whichIDToKeepAsZero):\n",
    "            if(len(tmpY)):\n",
    "                tmpY += ','\n",
    "            tmpY += str(edges_cut)\n",
    "            if(len(tmpG)):\n",
    "                tmpG += ','\n",
    "            tmpG += str(0)\n",
    "            \n",
    "        else:        \n",
    "            if(len(y)):\n",
    "                y += ','\n",
    "            y += str(edges_cut)\n",
    "            if(len(g)):\n",
    "                g += ','\n",
    "            g += str(i_key)\n",
    "            #g += '\"' + str(i_key) + '\"'\n",
    "            if(i == 40):\n",
    "                break\n",
    "\n",
    "y = \"Y_best <- c(\" + tmpY + ',' + y + \")\"\n",
    "g = \"g_best <- as.factor(c(\" + tmpG + ',' + g + \"))\"\n",
    "print(y)\n",
    "print(\"\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target best mode centrality: 61\n",
      "k_best <- c(\"Flow_betweenness_centrality_HL\",\"AA_random\",\"Alpha_HL\",\"Alpha_LH\",\"Average_distance_HL\",\"Average_distance_LH\",\"Barycenter_centrality_HL\",\"Barycenter_centrality_LH\",\"Betweenness_HL\",\"Betweenness_LH\",\"BottleNeck_centrality_HL\",\"BottleNeck_centrality_LH\",\"Bridging_centrality_HL\",\"Bridging_centrality_LH\",\"Centroid_centrality_HL\",\"Centroid_centrality_LH\",\"Closeness_Freeman_HL\",\"Closeness_Freeman_LH\",\"Closeness_VariantLatora_HL\",\"Closeness_VariantLatora_LH\",\"ClusterRank_HL\",\"ClusterRank_LH\",\"Communicability_betweenness_centrality_HL\",\"Communicability_betweenness_centrality_LH\",\"Community_centrality_HL\",\"Community_centrality_LH\",\"Core_decomposition_HL\",\"Core_decomposition_LH\",\"Cross_clique_centrality_LH\",\"Cross_clique_connectivity_HL\",\"Current_flow_closeness_centrality_HL\",\"Current_flow_closeness_centrality_LH\",\"Dangalchev_closeness_centrality_HL\",\"Dangalchev_closeness_centrality_LH\",\"Decay_centrality_HL\",\"Decay_centrality_LH\",\"Degree_centrality_HL\",\"Degree_centrality_LH\",\"Diffusion_degree_HL\",\"Diffusion_degree_LH\",\"DMNC_centrality_HL\",\"DMNC_centrality_LH\",\"Eccentricity_HL\",\"Eccentricity_LH\",\"Effectiveness_centrality_HL\",\"Effectiveness_centrality_LH\",\"Eigenvector_HL\",\"Eigenvector_LH\",\"Entropy_centrality_HL\",\"Entropy_centrality_LH\",\"EPC_HL\",\"EPC_LH\",\"Flow_betweenness_centrality_LH\",\"Information_centrality_HL\",\"Information_centrality_LH\",\"Kleinbergs_centrality_HITS_HL\",\"Kleinbergs_centrality_HITS_LH\",\"LAC_HL\",\"LAC_LH\",\"Lapacian_centrality_HL\",\"Lapacian_centrality_LH\",\"Leverage_centrality_HL\",\"Leverage_centrality_LH\",\"Lin_centrality_HL\",\"Lin_centrality_LH\",\"Load_centrality_HL\",\"Load_centrality_LH\",\"Lobby_index_HL\",\"Lobby_index_LH\",\"Local_assortativity_HL\",\"Local_assortativity_LH\",\"Local_clustering_coefficients_HL\",\"Local_clustering_coefficients_LH\",\"Markov_centrality_HL\",\"Markov_centrality_LH\",\"MCC_centrality_HL\",\"MCC_centrality_LH\",\"MNC_centrality_HL\",\"MNC_centrality_LH\",\"Neighborhood_connectivity_HL\",\"Neighborhood_connectivity_LH\",\"Network_centrality_HL\",\"Network_centrality_LH\",\"Network_fragmentation_GeodesicDistanceWeighted_HL\",\"Network_fragmentation_GeodesicDistanceWeighted_LH\",\"Network_fragmentation_HL\",\"Network_fragmentation_LH\",\"Path_centrality_HL\",\"Path_centrality_LH\",\"Political_independence_index_HL\",\"Political_independence_index_LH\",\"Radiality_centrality_HL\",\"Radiality_centrality_LH\",\"Random_walk_betweenness_HL\",\"Random_walk_betweenness_LH\",\"Random_walk_closeness_HL\",\"Random_walk_closeness_LH\",\"SALSA_HL\",\"SALSA_LH\",\"Semi_local_centrality_HL\",\"Semi_local_centrality_LH\",\"Shortest_path_betweenness_HL\",\"Shortest_path_betweenness_LH\",\"Shortest_path_closeness_HL\",\"Shortest_path_closeness_LH\",\"Shortest_path_degree_HL\",\"Shortest_path_degree_LH\",\"Strength_weighted_vertex_degree_HL\",\"Strength_weighted_vertex_degree_LH\",\"Stress_centrality_HL\",\"Stress_centrality_LH\",\"Subgraph_HL\",\"Subgraph_LH\",\"Topological_coefficient_HL\",\"Topological_coefficient_LH\")\n"
     ]
    }
   ],
   "source": [
    "k = ''\n",
    "for key in sorted(gkey.keys()):\n",
    "    if(len(k)):\n",
    "        k += ','\n",
    "    k += '\"' + gkey[key] + '\"'\n",
    "    if gkey[key] == 'Leverage_centrality_HL':\n",
    "        print('target best mode centrality:', key)\n",
    "\n",
    "k = 'k_best <- c(' + k + ')'\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# metrics important = \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def findBin(value, edges):\n",
    "    previousEdge = 0.0\n",
    "    for i, edge in enumerate(edges):\n",
    "        if i != 0:\n",
    "            if value >= previousEdge and value < edge:\n",
    "                return i\n",
    "        previousEdge = edge\n",
    "    return len(edges)\n",
    "\n",
    "def binScore(bin_id, num_bins, low_is_max = True):\n",
    "    half = int(num_bins * 0.5)\n",
    "    diff = 0\n",
    "    if(low_is_max):\n",
    "        if(bin_id > half):\n",
    "            return 0\n",
    "        diff = (half + 1 - bin_id)\n",
    "    else:\n",
    "        if(bin_id <= half):\n",
    "            return 0\n",
    "        diff = bin_id - half\n",
    "    return diff * 25 / half\n",
    "            \n",
    "        \n",
    "stats_metrics = [2, 3, 4, 5] # \"EDGES CUT\", \"TOTAL COMM VOLUME\", \"MODULARITY\", \"LONELINESS\"\n",
    "max_to_low_metrics = [4]\n",
    "\n",
    "means = {}\n",
    "stds = {}\n",
    "skews = {}\n",
    "\n",
    "means_hist = {}\n",
    "means_binedges = {}\n",
    "stds_hist = {}\n",
    "stds_binedges = {}\n",
    "skews_hist = {}\n",
    "skews_binedges = {}\n",
    "\n",
    "\n",
    "\n",
    "for stat in stats_metrics:\n",
    "    means[stat] = []\n",
    "    stds[stat] = []\n",
    "    skews[stat] = []\n",
    "    means_hist[stat] = {}\n",
    "    means_binedges[stat] = {}\n",
    "    stds_hist[stat] = {}\n",
    "    stds_binedges[stat] = {}\n",
    "    skews_hist[stat] = {}\n",
    "    skews_binedges[stat] = {}\n",
    "\n",
    "\n",
    "for key in list(centralities.keys()):\n",
    "    centrality = centralities[key]\n",
    "    centralityCode = centrality.centralityType + \":\" + centrality.orderType\n",
    "    \n",
    "    centrality.loadScores()\n",
    "    centrality.computeStatsScore()\n",
    "    \n",
    "    for smetric in stats_metrics:\n",
    "        means[smetric].append(centrality.avgScores[smetric])\n",
    "        stds[smetric].append(centrality.stdScores[smetric])\n",
    "        skews[smetric].append(centrality.skewnessScores[smetric])\n",
    "\n",
    "\n",
    "        \n",
    "for stat in stats_metrics:\n",
    "    means[stat] = np.array(means[stat])\n",
    "    stds[stat] = np.array(stds[stat])\n",
    "    skews[stat] = np.array(skews[stat])\n",
    "    \n",
    "    means_hist[stat], means_binedges[stat] = np.histogram(means[stat], bins='auto')\n",
    "    stds_hist[stat], stds_binedges[stat] = np.histogram(stds[stat], bins='auto')\n",
    "    skews_hist[stat], skews_binedges[stat] = np.histogram(skews[stat], bins='auto')\n",
    "\n",
    "rank = {}\n",
    "\n",
    "for key in list(centralities.keys()):\n",
    "    # compute scores for this statistic for each centrality\n",
    "    centrality = centralities[key]\n",
    "\n",
    "    centrality.totalScore = 0.0\n",
    "\n",
    "    for smetric in stats_metrics:\n",
    "        mu = centrality.avgScores[smetric]\n",
    "        std = centrality.stdScores[smetric]\n",
    "        skew = centrality.skewnessScores[smetric]\n",
    "\n",
    "        mean_bins = len(means_hist[stat])\n",
    "        std_bins = len(stds_hist[stat])\n",
    "        skew_bins = len(skews_hist[stat])\n",
    "\n",
    "        low_is_max = True\n",
    "        if smetric in max_to_low_metrics:\n",
    "            low_is_max = False\n",
    "\n",
    "        # mean should either be max or min\n",
    "        mu_score = binScore(findBin(mu, means_binedges[smetric]), mean_bins, low_is_max)\n",
    "        # std score should always be minimized\n",
    "        std_score = binScore(findBin(std, stds_binedges[smetric]), std_bins)\n",
    "        skew_score = binScore(findBin(skew, skews_binedges[smetric]), skew_bins, low_is_max)\n",
    "        \n",
    "        # override skew score\n",
    "        skew_score = 0.0\n",
    "        \n",
    "        centrality.totalScore += mu_score + std_score + skew_score\n",
    "    print(centrality.centralityType + \":\" + centrality.orderType, centrality.totalScore)\n",
    "    if centrality.totalScore in rank:\n",
    "        rank[centrality.totalScore].append(centrality.centralityType + \":\" + centrality.orderType)\n",
    "    else:\n",
    "        rank[centrality.totalScore] = [centrality.centralityType + \":\" + centrality.orderType]\n",
    "\n",
    "count = 0\n",
    "for key in sorted(rank, reverse=True):\n",
    "    for item in rank[key]:\n",
    "        count += 1\n",
    "        print(count, key, item)\n",
    "        if count == 20:\n",
    "            print(\"===================\")\n",
    "        \n",
    "for stat in stats_metrics:\n",
    "    break\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.hist(means[stat])\n",
    "\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.hist(stds[stat])\n",
    "\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.hist(skews[stat])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "randomCentrality = centralities[\"AA_random\"]\n",
    "\n",
    "mu_stats = {}\n",
    "std_stats = {}\n",
    "\n",
    "for stat in stats_metrics:\n",
    "    mu_stats[stat] = np.mean(means[stat])\n",
    "    std_stats[stat] = np.std(means[stat])\n",
    "    \n",
    "# z - scores\n",
    "for key in list(centralities.keys()):\n",
    "    # compute scores for this statistic for each centrality\n",
    "    centrality = centralities[key]\n",
    "    if key == \"AA_random\":\n",
    "        continue\n",
    "    \n",
    "    # compute z-score\n",
    "    stats_metrics = [2, 3, 4, 5] # \"EDGES CUT\", \"TOTAL COMM VOLUME\", \"MODULARITY\", \"LONELINESS\"\n",
    "\n",
    "    print(\"Z-SCORE: \", centrality.centralityType + \":\" + centrality.orderType)\n",
    "    \n",
    "    for stat in stats_metrics:\n",
    "        x = centrality.avgScores[stat]\n",
    "        \n",
    "        zscore = (x - mu_stats[stat]) / std_stats[stat]\n",
    "        tabs = \"\\t\\t\"\n",
    "        if stat == 3:\n",
    "            tabs = \"\\t\"\n",
    "        print(\"   \", cols[stat], tabs, \"{0:.5f}\".format(zscore), \"\\t\", \"{0:.5f}\".format(sstats.norm.cdf(zscore)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
