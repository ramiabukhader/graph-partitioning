{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import platform\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from graph_partitioning import GraphPartitioning, utils\n",
    "\n",
    "run_metrics = True\n",
    "\n",
    "cols = [\"WASTE\", \"CUT RATIO\", \"EDGES CUT\", \"TOTAL COMM VOLUME\", \"Qds\", \"CONDUCTANCE\", \"MAXPERM\", \"RBSE\", \"NMI\", \"FSCORE\", \"FSCORE RELABEL IMPROVEMENT\", \"LONELINESS\"]\n",
    "\n",
    "pwd = %pwd\n",
    "\n",
    "config = {\n",
    "\n",
    "    \"DATA_FILENAME\": os.path.join(pwd, \"data\", \"predition_model_tests\", \"network\", \"network_$$.txt\"),\n",
    "    \"OUTPUT_DIRECTORY\": os.path.join(pwd, \"output\"),\n",
    "\n",
    "    # Set which algorithm is run for the PREDICTION MODEL.\n",
    "    # Either: 'FENNEL' or 'SCOTCH'\n",
    "    \"PREDICTION_MODEL_ALGORITHM\": \"FENNEL\",\n",
    "\n",
    "    # Alternativly, read input file for prediction model.\n",
    "    # Set to empty to generate prediction model using algorithm value above.\n",
    "    \"PREDICTION_MODEL\": \"\",\n",
    "\n",
    "    \n",
    "    \"PARTITIONER_ALGORITHM\": \"FENNEL\",\n",
    "\n",
    "    # File containing simulated arrivals. This is used in simulating nodes\n",
    "    # arriving at the shelter. Nodes represented by line number; value of\n",
    "    # 1 represents a node as arrived; value of 0 represents the node as not\n",
    "    # arrived or needing a shelter.\n",
    "    \"SIMULATED_ARRIVAL_FILE\": os.path.join(pwd,\n",
    "                                           \"data\",\n",
    "                                           \"predition_model_tests\",\n",
    "                                           \"#scenario#\",\n",
    "                                           \"simulated_arrival_list\",\n",
    "                                           \"percentage_of_prediction_correct_££\",\n",
    "                                           \"arrival_££_$$.txt\"\n",
    "                                          ),\n",
    "\n",
    "    # File containing the prediction of a node arriving. This is different to the\n",
    "    # simulated arrivals, the values in this file are known before the disaster.\n",
    "    \"PREDICTION_LIST_FILE\": os.path.join(pwd,\n",
    "                                         \"data\",\n",
    "                                         \"predition_model_tests\",\n",
    "                                         \"#scenario#\",\n",
    "                                         \"prediction_list\",\n",
    "                                         \"prediction_$$.txt\"\n",
    "                                        ),\n",
    "\n",
    "    # File containing the geographic location of each node, in \"x,y\" format.\n",
    "    \"POPULATION_LOCATION_FILE\": os.path.join(pwd,\n",
    "                                             \"data\",\n",
    "                                             \"predition_model_tests\",\n",
    "                                             \"coordinates\",\n",
    "                                             \"coordinates_$$.txt\"\n",
    "                                            ),\n",
    "\n",
    "    # Number of shelters\n",
    "    \"num_partitions\": 4,\n",
    "\n",
    "    # The number of iterations when making prediction model\n",
    "    \"num_iterations\": 10,\n",
    "\n",
    "    # Percentage of prediction model to use before discarding\n",
    "    # When set to 0, prediction model is discarded, useful for one-shot\n",
    "    \"prediction_model_cut_off\": 0.0,\n",
    "\n",
    "    # Alpha value used in one-shot (when restream_batches set to 1)\n",
    "    \"one_shot_alpha\": 0.5,\n",
    "    \n",
    "    \"use_one_shot_alpha\" : False,\n",
    "    \n",
    "    # Number of arrivals to batch before recalculating alpha and restreaming.\n",
    "    \"restream_batches\": 50,\n",
    "\n",
    "    # When the batch size is reached: if set to True, each node is assigned\n",
    "    # individually as first in first out. If set to False, the entire batch\n",
    "    # is processed and empty before working on the next batch.\n",
    "    \"sliding_window\": False,\n",
    "\n",
    "    # Create virtual nodes based on prediction model\n",
    "    \"use_virtual_nodes\": False,\n",
    "\n",
    "    # Virtual nodes: edge weight\n",
    "    \"virtual_edge_weight\": 1.0,\n",
    "    \n",
    "    # Loneliness score parameter. Used when scoring a partition by how many\n",
    "    # lonely nodes exist.\n",
    "    \"loneliness_score_param\": 1.2,\n",
    "\n",
    "    \"compute_metrics_enabled\": True,\n",
    "\n",
    "    ####\n",
    "    # GRAPH MODIFICATION FUNCTIONS\n",
    "\n",
    "    # Also enables the edge calculation function.\n",
    "    \"graph_modification_functions\": True,\n",
    "\n",
    "    # If set, the node weight is set to 100 if the node arrives at the shelter,\n",
    "    # otherwise the node is removed from the graph.\n",
    "    \"alter_arrived_node_weight_to_100\": False,\n",
    "\n",
    "    # Uses generalized additive models from R to generate prediction of nodes not\n",
    "    # arrived. This sets the node weight on unarrived nodes the the prediction\n",
    "    # given by a GAM.\n",
    "    # Needs POPULATION_LOCATION_FILE to be set.\n",
    "    \"alter_node_weight_to_gam_prediction\": False,\n",
    "    \n",
    "    # Enables edge expansion when graph_modification_functions is set to true\n",
    "    \"edge_expansion_enabled\": True,\n",
    "\n",
    "    # The value of 'k' used in the GAM will be the number of nodes arrived until\n",
    "    # it reaches this max value.\n",
    "    \"gam_k_value\": 100,\n",
    "\n",
    "    # Alter the edge weight for nodes that haven't arrived. This is a way to\n",
    "    # de-emphasise the prediction model for the unknown nodes.\n",
    "    \"prediction_model_emphasis\": 1.0,\n",
    "    \n",
    "    # This applies the prediction_list_file node weights onto the nodes in the graph\n",
    "    # when the prediction model is being computed and then removes the weights\n",
    "    # for the cutoff and batch arrival modes\n",
    "    \"apply_prediction_model_weights\": True,\n",
    "\n",
    "    \"SCOTCH_LIB_PATH\": os.path.join(pwd, \"libs/scotch/macOS/libscotch.dylib\")\n",
    "    if 'Darwin' in platform.system()\n",
    "    else \"/usr/local/lib/libscotch.so\",\n",
    "    \n",
    "    # Path to the PaToH shared library\n",
    "    \"PATOH_LIB_PATH\": os.path.join(pwd, \"libs/patoh/lib/macOS/libpatoh.dylib\")\n",
    "    if 'Darwin' in platform.system()\n",
    "    else os.path.join(pwd, \"libs/patoh/lib/linux/libpatoh.so\"),\n",
    "    \n",
    "    \"PATOH_ITERATIONS\": 5,\n",
    "        \n",
    "    # Expansion modes: 'avg_node_weight', 'total_node_weight', 'smallest_node_weight'\n",
    "    # 'largest_node_weight'\n",
    "    # add '_squared' or '_sqrt' at the end of any of the above for ^2 or sqrt(weight)\n",
    "    # i.e. 'avg_node_weight_squared\n",
    "    \"PATOH_HYPEREDGE_EXPANSION_MODE\": 'no_expansion',\n",
    "    \n",
    "    # Edge Expansion: average, total, minimum, maximum, product, product_squared, sqrt_product\n",
    "    \"EDGE_EXPANSION_MODE\" : 'total',\n",
    "    \n",
    "    # Whether nodes should be reordered using a centrality metric for optimal node assignments in batch mode\n",
    "    # This is specific to FENNEL and at the moment Leverage Centrality is used to compute new noder orders\n",
    "    \"FENNEL_NODE_REORDERING_ENABLED\": False,\n",
    "    \n",
    "    # Whether the Friend of a Friend scoring system is active during FENNEL partitioning.\n",
    "    # FOAF employs information about a node's friends to determine the best partition when\n",
    "    # this node arrives at a shelter and no shelter has friends already arrived\n",
    "    \"FENNEL_FRIEND_OF_A_FRIEND_ENABLED\": False,\n",
    "    \n",
    "    # Alters how much information to print. Keep it at 1 for this notebook.\n",
    "    # 0 - will print nothing, useful for batch operations.\n",
    "    # 1 - prints basic information on assignments and operations.\n",
    "    # 2 - prints more information as it batches arrivals.\n",
    "    \"verbose\": 1\n",
    "}\n",
    "\n",
    "gp = GraphPartitioning(config)\n",
    "\n",
    "# Optional: shuffle the order of nodes arriving\n",
    "# Arrival order should not be shuffled if using GAM to alter node weights\n",
    "#random.shuffle(gp.arrival_order)\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario dataset_1_shift_rotate\n",
      "Scenario dataset_1_shift_rotate 100 Iteration 0\n",
      "Average Elapsed Time = 7.00355005264\n",
      "CUT_RATIO,dataset_1_shift_rotate_100,0.175,0.0,0.175\n",
      "EC,dataset_1_shift_rotate_100,105.0,0.0,105\n",
      "TCV,dataset_1_shift_rotate_100,141.0,0.0,141\n",
      "LONELINESS,dataset_1_shift_rotate_100,0.756318276461,0.0,0.756318276461\n",
      "QDS,dataset_1_shift_rotate_100,0.472432948788,0.0,0.4724329487876044\n",
      "CONDUCTANCE,dataset_1_shift_rotate_100,0.0603615377181,0.0,0.06036153771810973\n",
      "MAXPERM,dataset_1_shift_rotate_100,0.337412637011,0.0,0.33741263701067625\n",
      "RBSE,dataset_1_shift_rotate_100,0.0604982206406,0.0,0.060498220640569395\n",
      "NMI,dataset_1_shift_rotate_100,0.0896011832858,0.0,0.0896011832858\n",
      "FSCORE,dataset_1_shift_rotate_100,0.195444407405,0.0,0.195444407405\n",
      "FSCORE_IMPROVE,dataset_1_shift_rotate_100,0.120300929527,0.0,0.120300929527\n",
      "Scenario dataset_1_shift_rotate\n",
      "Scenario dataset_1_shift_rotate 80 Iteration 0\n",
      "Average Elapsed Time = 6.46811914444\n",
      "CUT_RATIO,dataset_1_shift_rotate_80,0.173310225303,0.0,0.173310225303\n",
      "EC,dataset_1_shift_rotate_80,100.0,0.0,100\n",
      "TCV,dataset_1_shift_rotate_80,137.0,0.0,137\n",
      "LONELINESS,dataset_1_shift_rotate_80,0.748523839873,0.0,0.748523839873\n",
      "QDS,dataset_1_shift_rotate_80,0.463491961611,0.0,0.4634919616112253\n",
      "CONDUCTANCE,dataset_1_shift_rotate_80,0.0485971244165,0.0,0.04859712441648762\n",
      "MAXPERM,dataset_1_shift_rotate_80,0.365544695971,0.0,0.36554469597069594\n",
      "RBSE,dataset_1_shift_rotate_80,0.032967032967,0.0,0.03296703296703297\n",
      "NMI,dataset_1_shift_rotate_80,0.142003479742,0.0,0.142003479742\n",
      "FSCORE,dataset_1_shift_rotate_80,0.191570288182,0.0,0.191570288182\n",
      "FSCORE_IMPROVE,dataset_1_shift_rotate_80,0.252421723691,0.0,0.252421723691\n",
      "Scenario dataset_1_shift_rotate\n",
      "Scenario dataset_1_shift_rotate 60 Iteration 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7a507b625894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_cut_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_arrival\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/voreno/Development/CSAP/graph-partitioning/graph_partitioning/graph_partitioning.py\u001b[0m in \u001b[0;36mbatch_arrival\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# batch processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestream_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_arrived\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mrun_metrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_arrived\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                     \u001b[0mbatch_arrived\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/voreno/Development/CSAP/graph-partitioning/graph_partitioning/graph_partitioning.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(self, batch_arrived, assign_all)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGsub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/voreno/Development/CSAP/graph-partitioning/graph_partitioning/graph_partitioning.py\u001b[0m in \u001b[0;36m_print_score\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m#q_qds_conductance = utils.louvainModularityComQuality(graph, self.assignments, self.num_partitions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# non-overlapping metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mq_qds_conductance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfomapModularityComQuality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mloneliness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloneliness_score_wavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloneliness_score_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/voreno/Development/CSAP/graph-partitioning/graph_partitioning/utils.py\u001b[0m in \u001b[0;36minfomapModularityComQuality\u001b[0;34m(G, assignments, num_partitions)\u001b[0m\n\u001b[1;32m    297\u001b[0m             retval = subprocess.call(\n\u001b[1;32m    298\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcom_qual_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 stdout=logwriter, stderr=subprocess.STDOUT)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mn_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1434\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1437\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "iterations = 100\n",
    "\n",
    "#pm_cutoff = []\n",
    "#for i in range(0, 21):\n",
    "#    pm_cutoff.append(i * 0.05)\n",
    "\n",
    "#virtual_edge_weight = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "\n",
    "correctednesses = [100, 80, 60, 40, 20, 0]\n",
    "scenarios = ['dataset_1_shift_rotate', 'dataset_2_shift_move', 'dataset_3_increase_prediction_perc'\n",
    "            , 'dataset_4_increase_total_perc', 'dataset_5_increase_total_perc_equals_total_prediction_prec',\n",
    "            'dataset_6_randomize']\n",
    "\n",
    "scenario = scenarios[0]\n",
    "\n",
    "for correctedness in correctednesses:\n",
    "    metricsDataPrediction = []\n",
    "    metricsDataAssign = []\n",
    "    \n",
    "    # batches of 50 - use 10 restreaming iterations on FENNEL\n",
    "    config['num_iterations'] = 10\n",
    "    config['PREDICTION_MODEL'] = os.path.join(pwd, \"data\", \"predition_model_tests\", \"network\", \"pm\", \"network_pm_$$.txt\")\n",
    "\n",
    "\n",
    "    print('Scenario', scenario)\n",
    "    elapsed_times = []\n",
    "    for i in range(0, iterations):\n",
    "        # how many networks\n",
    "        if (i % 20) == 0:\n",
    "            print('Scenario', scenario, correctedness, 'Iteration', str(i))\n",
    "        \n",
    "        conf = deepcopy(config)\n",
    "        \n",
    "        conf[\"DATA_FILENAME\"] = conf[\"DATA_FILENAME\"].replace('$$', str(i + 1))\n",
    "        \n",
    "        conf[\"SIMULATED_ARRIVAL_FILE\"] = conf[\"SIMULATED_ARRIVAL_FILE\"].replace('$$', str(i + 1))\n",
    "        conf[\"SIMULATED_ARRIVAL_FILE\"] = conf[\"SIMULATED_ARRIVAL_FILE\"].replace('££', str(correctedness))\n",
    "        conf[\"SIMULATED_ARRIVAL_FILE\"] = conf[\"SIMULATED_ARRIVAL_FILE\"].replace('#scenario#', str(scenario))\n",
    "\n",
    "        conf[\"PREDICTION_LIST_FILE\"] = conf[\"PREDICTION_LIST_FILE\"].replace('$$', str(i + 1))\n",
    "        conf[\"PREDICTION_LIST_FILE\"] = conf[\"PREDICTION_LIST_FILE\"].replace('#scenario#', scenario)\n",
    "\n",
    "        conf[\"POPULATION_LOCATION_FILE\"] = conf[\"POPULATION_LOCATION_FILE\"].replace('$$', str(i + 1))\n",
    "        conf['PREDICTION_MODEL'] = conf['PREDICTION_MODEL'].replace('$$', str(i + 1))\n",
    "\n",
    "\n",
    "        with GraphPartitioning(conf) as gp:\n",
    "            gp.verbose = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            gp.load_network()\n",
    "            gp.init_partitioner()\n",
    "\n",
    "            m = gp.prediction_model()\n",
    "            m = gp.assign_cut_off()\n",
    "            m = gp.batch_arrival()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            elapsed_times.append(elapsed_time)\n",
    "            \n",
    "            totalM = len(m)\n",
    "            metricsDataPrediction.append(m[totalM - 1])\n",
    "    \n",
    "    print('Average Elapsed Time =', scipy.mean(elapsed_times))\n",
    "\n",
    "    waste = ''\n",
    "    cutratio = ''\n",
    "    ec = ''\n",
    "    tcv = ''\n",
    "    qds = ''\n",
    "    conductance = ''\n",
    "    maxperm = ''\n",
    "    rbse = ''\n",
    "    nmi = ''\n",
    "    lonliness = ''\n",
    "    fscore = ''\n",
    "    fscoreimprove = ''\n",
    "        \n",
    "    qdsOv = ''\n",
    "    condOv = ''\n",
    "\n",
    "    dataWaste = []\n",
    "    dataCutRatio = []\n",
    "    dataEC = []\n",
    "    dataTCV = [] \n",
    "    dataQDS = []\n",
    "    dataCOND = []\n",
    "    dataMAXPERM = []\n",
    "    dataRBSE = []\n",
    "    dataNMI = []\n",
    "    dataLonliness = []\n",
    "    dataFscore = []\n",
    "    dataFscoreImprove = []\n",
    "\n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        dataWaste.append(metricsDataPrediction[i][0])        \n",
    "        dataCutRatio.append(metricsDataPrediction[i][1])\n",
    "        dataEC.append(metricsDataPrediction[i][2])\n",
    "        dataTCV.append(metricsDataPrediction[i][3])\n",
    "        dataQDS.append(metricsDataPrediction[i][4])\n",
    "        dataCOND.append(metricsDataPrediction[i][5])\n",
    "        dataMAXPERM.append(metricsDataPrediction[i][6])\n",
    "        dataRBSE.append(metricsDataPrediction[i][7])\n",
    "        dataNMI.append(metricsDataPrediction[i][8])        \n",
    "        dataFscore.append(metricsDataPrediction[i][9])        \n",
    "        dataFscoreImprove.append(metricsDataPrediction[i][10])        \n",
    "        dataLonliness.append(metricsDataPrediction[i][11])\n",
    "\n",
    "\n",
    "        if(len(waste)):\n",
    "            waste = waste + ','\n",
    "        waste = waste + str(metricsDataPrediction[i][0])\n",
    "\n",
    "        if(len(cutratio)):\n",
    "            cutratio = cutratio + ','\n",
    "        cutratio = cutratio + str(metricsDataPrediction[i][1])\n",
    "\n",
    "        if(len(ec)):\n",
    "            ec = ec + ','\n",
    "        ec = ec + str(metricsDataPrediction[i][2])\n",
    "        \n",
    "        if(len(tcv)):\n",
    "            tcv = tcv + ','\n",
    "        tcv = tcv + str(metricsDataPrediction[i][3])\n",
    "\n",
    "        if(len(qds)):\n",
    "            qds = qds + ','\n",
    "        qds = qds + str(metricsDataPrediction[i][4])\n",
    "\n",
    "        if(len(conductance)):\n",
    "            conductance = conductance + ','\n",
    "        conductance = conductance + str(metricsDataPrediction[i][5])\n",
    "\n",
    "        if(len(maxperm)):\n",
    "            maxperm = maxperm + ','\n",
    "        maxperm = maxperm + str(metricsDataPrediction[i][6])\n",
    "\n",
    "        if(len(rbse)):\n",
    "            rbse = rbse + ','\n",
    "        rbse = rbse + str(metricsDataPrediction[i][7])\n",
    "\n",
    "\n",
    "        \n",
    "        if(len(nmi)):\n",
    "            nmi = nmi + ','\n",
    "        nmi = nmi + str(metricsDataPrediction[i][8])\n",
    "\n",
    "        if(len(fscore)):\n",
    "            fscore = fscore + ','\n",
    "        fscore = fscore + str(metricsDataPrediction[i][9])\n",
    "\n",
    "        if(len(fscoreimprove)):\n",
    "            fscoreimprove = fscoreimprove + ','\n",
    "        fscoreimprove = fscoreimprove + str(metricsDataPrediction[i][10])\n",
    "        \n",
    "        if(len(lonliness)):\n",
    "            lonliness = lonliness + ','\n",
    "        lonliness = lonliness + str(dataLonliness[i])\n",
    "        \n",
    "\n",
    "    waste = 'WASTE,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataWaste)) + ',' + str(scipy.std(dataWaste)) + ',' + waste\n",
    "\n",
    "    cutratio = 'CUT_RATIO,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataCutRatio)) + ',' + str(scipy.std(dataCutRatio)) + ',' + cutratio\n",
    "    ec = 'EC,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataEC)) + ',' + str(scipy.std(dataEC)) + ',' + ec\n",
    "    tcv = 'TCV,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataTCV)) + ',' + str(scipy.std(dataTCV)) + ',' + tcv\n",
    "\n",
    "    lonliness = \"LONELINESS,\" + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataLonliness)) + ',' + str(scipy.std(dataLonliness)) + ',' + lonliness\n",
    "\n",
    "    qds = 'QDS,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataQDS)) + ',' + str(scipy.std(dataQDS)) + ',' + qds\n",
    "    conductance = 'CONDUCTANCE,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataCOND)) + ',' + str(scipy.std(dataCOND)) + ',' + conductance\n",
    "    maxperm = 'MAXPERM,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataMAXPERM)) + ',' + str(scipy.std(dataMAXPERM)) + ',' + maxperm\n",
    "    rbse = 'RBSE,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataRBSE)) + ',' + str(scipy.std(dataRBSE)) + ',' + rbse\n",
    "\n",
    "    nmi = 'NMI,' + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataNMI)) + ',' + str(scipy.std(dataNMI)) + ',' + nmi\n",
    "\n",
    "    fscore = \"FSCORE,\" + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataFscore)) + ',' + str(scipy.std(dataFscore)) + ',' + fscore\n",
    "    fscoreimprove = \"FSCORE_IMPROVE,\" + scenario + '_' + str(correctedness) + ',' + str(scipy.mean(dataFscoreImprove)) + ',' + str(scipy.std(dataFscoreImprove)) + ',' + fscoreimprove\n",
    "\n",
    "\n",
    "    print(cutratio)\n",
    "    print(ec)\n",
    "    print(tcv)\n",
    "    print(lonliness)\n",
    "    print(qds)\n",
    "    print(conductance)\n",
    "    print(maxperm)\n",
    "    print(rbse)\n",
    "    print(nmi)\n",
    "    print(fscore)\n",
    "    print(fscoreimprove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
